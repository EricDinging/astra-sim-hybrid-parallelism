[ASTRA-sim] Compiling ASTRA-sim with the Analytical Network Backend...

-- {fmt} version: 11.0.2
-- Build type: Release
-- Build spdlog: 1.14.1
-- Build type: Release
-- Configuring done
-- Generating done
-- Build files have been written to: /app/astra-sim/build/astra_analytical/build
Consolidate compiler generated dependencies of target fmt
Consolidate compiler generated dependencies of target spdlog
Consolidate compiler generated dependencies of target yaml-cpp
[  2%] Built target fmt
[  6%] Built target spdlog
[ 25%] Built target yaml-cpp
Consolidate compiler generated dependencies of target yaml-cpp-parse
Consolidate compiler generated dependencies of target yaml-cpp-sandbox
Consolidate compiler generated dependencies of target yaml-cpp-read
Consolidate compiler generated dependencies of target Analytical_Congestion_Unaware
Consolidate compiler generated dependencies of target Analytical_Reconfigurable
Consolidate compiler generated dependencies of target Analytical_Congestion_Aware
[ 26%] Built target yaml-cpp-parse
[ 28%] Built target yaml-cpp-sandbox
[ 29%] Built target yaml-cpp-read
[ 37%] Built target Analytical_Congestion_Unaware
[ 45%] Built target Analytical_Reconfigurable
[ 52%] Built target Analytical_Congestion_Aware
Consolidate compiler generated dependencies of target AstraSim
[ 84%] Built target AstraSim
Consolidate compiler generated dependencies of target AstraSim_Analytical_Congestion_Unaware
Consolidate compiler generated dependencies of target AstraSim_Analytical_Congestion_Aware
Consolidate compiler generated dependencies of target AstraSim_Analytical_Reconfigurable
[ 90%] Built target AstraSim_Analytical_Congestion_Unaware
[ 98%] Built target AstraSim_Analytical_Congestion_Aware
[100%] Built target AstraSim_Analytical_Reconfigurable

[ASTRA-sim] Compilation finished.
[ASTRA-sim] Running ASTRA-sim Example with Analytical Network Backend...

[2025-08-29 17:41:44.308] [system::topology::RingTopology] [info] ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1 total nodes in ring: 2
[2025-08-29 17:41:44.308] [system::topology::RingTopology] [info] ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1 total nodes in ring: 2
[2025-08-29 17:41:44.308] [system::topology::RingTopology] [info] ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1 total nodes in ring: 2
Message: [2025-08-29 17:41:44.308] [system::topology::RingTopology] [info] ring of node 0, id: 0 dimension: local total nodes in ring: 2 index in ring: 0 offset: 1 total nodes in ring: 2
attr {
  name: "schema"
  string_val: "1.1.1-chakra.0.0.4"
}
attr {
  name: "pid"
  uint64_val: 2957443
}
attr {
  name: "time"
  string_val: "2025-07-19 00:16:27"
}
attr {
  name: "start_ts"
  uint64_val: 6244489396
}
attr {
  name: "finish_ts"
  uint64_val: 6244490592
}

Message: id: 1
name: "[pytorch|profiler|execution_trace|thread]"
type: COMP_NODE
ctrl_deps: 0
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 0
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 1: 
Adding parent for node 1: 0 
Adding parent for node 1: 0 
Adding node 1
Message: id: 2
name: "ProfilerStep#0"
type: COMP_NODE
ctrl_deps: 1
data_deps: 1
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 1
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 2: 1 
Adding parent for node 2: 1 
Adding parent for node 2: 1 
Adding node 2
Message: id: 3
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 2
data_deps: 2
inputs {
  values: "[[16, 8], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[5, 6, 0, 128, 4, \'cpu\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 2
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 3: 2 
Adding parent for node 3: 2 
Adding parent for node 3: 2 
Adding node 3
Message: id: 4
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 3
inputs {
  values: "[[5, 6, 0, 128, 4, \'cpu\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 3
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 4: 3 
Adding parent for node 4: 2 
Adding parent for node 4: 2 3 
Adding node 4
Message: id: 5
name: "detach"
type: COMP_NODE
ctrl_deps: 4
data_deps: 4
inputs {
  values: "[[5, 6, 0, 128, 4, \'cpu\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 4
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 5: 4 
Adding parent for node 5: 4 
Adding parent for node 5: 4 
Adding node 5
Message: id: 6
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 2
data_deps: 5
inputs {
  values: "[[16], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[5, 11, 0, 16, 4, \'cpu\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 5
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 6: 5 
Adding parent for node 6: 2 
Adding parent for node 6: 2 5 
Adding node 6
Message: id: 7
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 6
inputs {
  values: "[[5, 11, 0, 16, 4, \'cpu\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 6
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 7: 6 
Adding parent for node 7: 2 
Adding parent for node 7: 2 6 
Adding node 7
Message: id: 8
name: "detach"
type: COMP_NODE
ctrl_deps: 7
data_deps: 7
inputs {
  values: "[[5, 11, 0, 16, 4, \'cpu\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 7
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 8: 7 
Adding parent for node 8: 7 
Adding parent for node 8: 7 
Adding node 8
Message: id: 9
name: "aten::uniform_"
type: COMP_NODE
ctrl_deps: 2
data_deps: 8
inputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\'], -0.353553, 0.353553, \'<None>\']"
  shapes: "[[16, 8], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 8
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::uniform_(Tensor(a!) self, float from=0., float to=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 9: 8 
Adding parent for node 9: 2 
Adding parent for node 9: 2 8 
Adding node 9
Message: id: 10
name: "aten::uniform_"
type: COMP_NODE
ctrl_deps: 2
data_deps: 9
inputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\'], -0.353553, 0.353553, \'<None>\']"
  shapes: "[[16], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 9
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::uniform_(Tensor(a!) self, float from=0., float to=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 10: 9 
Adding parent for node 10: 2 
Adding parent for node 10: 2 9 
Adding node 10
Message: id: 11
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 2
data_deps: 10
inputs {
  values: "[[4, 16], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[18, 19, 0, 64, 4, \'cpu\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 10
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 11: 10 
Adding parent for node 11: 2 
Adding parent for node 11: 2 10 
Adding node 11
Message: id: 12
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 11
inputs {
  values: "[[18, 19, 0, 64, 4, \'cpu\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 11
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 12: 11 
Adding parent for node 12: 2 
Adding parent for node 12: 2 11 
Adding node 12
Message: id: 13
name: "detach"
type: COMP_NODE
ctrl_deps: 12
data_deps: 12
inputs {
  values: "[[18, 19, 0, 64, 4, \'cpu\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 12
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 13: 12 
Adding parent for node 13: 12 
Adding parent for node 13: 12 
Adding node 13
Message: id: 14
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 2
data_deps: 13
inputs {
  values: "[[4], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[18, 24, 0, 4, 4, \'cpu\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 13
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 14: 13 
Adding parent for node 14: 2 
Adding parent for node 14: 2 13 
Adding node 14
Message: id: 15
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 14
inputs {
  values: "[[18, 24, 0, 4, 4, \'cpu\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 14
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 15: 14 
Adding parent for node 15: 2 
Adding parent for node 15: 2 14 
Adding node 15
Message: id: 16
name: "detach"
type: COMP_NODE
ctrl_deps: 15
data_deps: 15
inputs {
  values: "[[18, 24, 0, 4, 4, \'cpu\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 15
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 16: 15 
Adding parent for node 16: 15 
Adding parent for node 16: 15 
Adding node 16
Message: id: 17
name: "aten::uniform_"
type: COMP_NODE
ctrl_deps: 2
data_deps: 16
inputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\'], -0.25, 0.25, \'<None>\']"
  shapes: "[[4, 16], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 16
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::uniform_(Tensor(a!) self, float from=0., float to=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 17: 16 
Adding parent for node 17: 2 
Adding parent for node 17: 2 16 
Adding node 17
Message: id: 18
name: "aten::uniform_"
type: COMP_NODE
ctrl_deps: 2
data_deps: 17
inputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\'], -0.25, 0.25, \'<None>\']"
  shapes: "[[4], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 17
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::uniform_(Tensor(a!) self, float from=0., float to=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 18: 17 
Adding parent for node 18: 2 
Adding parent for node 18: 2 17 
Adding node 18
Message: id: 19
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 18
inputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[16, 8], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 18
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 19: 18 
Adding parent for node 19: 2 
Adding parent for node 19: 2 18 
Adding node 19
Message: id: 20
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 19
data_deps: 19
inputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[16, 8], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 19
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 20: 19 
Adding parent for node 20: 19 
Adding parent for node 20: 19 
Adding node 20
Message: id: 21
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 20
data_deps: 20
inputs {
  values: "[[16, 8], [8, 1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[], []], [[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 20
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 21: 20 
Adding parent for node 21: 20 
Adding parent for node 21: 20 
Adding node 21
Message: id: 22
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 20
data_deps: 21
inputs {
  values: "[[33, 34, 0, 128, 4, \'cuda:0\'], [9, 6, 0, 128, 4, \'cpu\'], False]"
  shapes: "[[16, 8], [16, 8], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 21
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 22: 21 
Adding parent for node 22: 20 
Adding parent for node 22: 20 21 
Adding node 22
Message: id: 23
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 22
inputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\'], [33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8], [16, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 22
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 23: 22 
Adding parent for node 23: 2 
Adding parent for node 23: 2 22 
Adding node 23
Message: id: 24
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 23
inputs {
  values: "[[9, 6, 0, 128, 4, \'cpu\'], [33, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8], [16, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 23
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 24: 23 
Adding parent for node 24: 2 
Adding parent for node 24: 2 23 
Adding node 24
Message: id: 25
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 24
inputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[16], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 24
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 25: 24 
Adding parent for node 25: 2 
Adding parent for node 25: 2 24 
Adding node 25
Message: id: 26
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 25
data_deps: 25
inputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[16], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 25
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 26: 25 
Adding parent for node 26: 25 
Adding parent for node 26: 25 
Adding node 26
Message: id: 27
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 26
data_deps: 26
inputs {
  values: "[[16], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 26
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 27: 26 
Adding parent for node 27: 26 
Adding parent for node 27: 26 
Adding node 27
Message: id: 28
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 26
data_deps: 27
inputs {
  values: "[[41, 42, 0, 16, 4, \'cuda:0\'], [14, 11, 0, 16, 4, \'cpu\'], False]"
  shapes: "[[16], [16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 27
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 28: 27 
Adding parent for node 28: 26 
Adding parent for node 28: 26 27 
Adding node 28
Message: id: 29
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 28
inputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\'], [41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 28
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 29: 28 
Adding parent for node 29: 2 
Adding parent for node 29: 2 28 
Adding node 29
Message: id: 30
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 29
inputs {
  values: "[[14, 11, 0, 16, 4, \'cpu\'], [41, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 29
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 30: 29 
Adding parent for node 30: 2 
Adding parent for node 30: 2 29 
Adding node 30
Message: id: 31
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 30
inputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[4, 16], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 30
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 31: 30 
Adding parent for node 31: 2 
Adding parent for node 31: 2 30 
Adding node 31
Message: id: 32
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 31
data_deps: 31
inputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[4, 16], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 31
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 32: 31 
Adding parent for node 32: 31 
Adding parent for node 32: 31 
Adding node 32
Message: id: 33
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 32
data_deps: 32
inputs {
  values: "[[4, 16], [16, 1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[], []], [[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 32
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 33: 32 
Adding parent for node 33: 32 
Adding parent for node 33: 32 
Adding node 33
Message: id: 34
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 32
data_deps: 33
inputs {
  values: "[[41, 49, 0, 64, 4, \'cuda:0\'], [22, 19, 0, 64, 4, \'cpu\'], False]"
  shapes: "[[4, 16], [4, 16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 33
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 34: 33 
Adding parent for node 34: 32 
Adding parent for node 34: 32 33 
Adding node 34
Message: id: 35
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 34
inputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\'], [41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 34
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 35: 34 
Adding parent for node 35: 2 
Adding parent for node 35: 2 34 
Adding node 35
Message: id: 36
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 35
inputs {
  values: "[[22, 19, 0, 64, 4, \'cpu\'], [41, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 35
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 36: 35 
Adding parent for node 36: 2 
Adding parent for node 36: 2 35 
Adding node 36
Message: id: 37
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 36
inputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[4], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 36
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 37: 36 
Adding parent for node 37: 2 
Adding parent for node 37: 2 36 
Adding node 37
Message: id: 38
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 37
data_deps: 37
inputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[4], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 37
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 38: 37 
Adding parent for node 38: 37 
Adding parent for node 38: 37 
Adding node 38
Message: id: 39
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 38
data_deps: 38
inputs {
  values: "[[4], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 38
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 39: 38 
Adding parent for node 39: 38 
Adding parent for node 39: 38 
Adding node 39
Message: id: 40
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 38
data_deps: 39
inputs {
  values: "[[33, 56, 0, 4, 4, \'cuda:0\'], [27, 24, 0, 4, 4, \'cpu\'], False]"
  shapes: "[[4], [4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 39
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 40: 39 
Adding parent for node 40: 38 
Adding parent for node 40: 38 39 
Adding node 40
Message: id: 41
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 40
inputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\'], [33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 40
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 41: 40 
Adding parent for node 41: 2 
Adding parent for node 41: 2 40 
Adding node 41
Message: id: 42
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 2
data_deps: 41
inputs {
  values: "[[27, 24, 0, 4, 4, \'cpu\'], [33, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 41
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 42: 41 
Adding parent for node 42: 2 
Adding parent for node 42: 2 41 
Adding node 42
Message: id: 43
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 42
inputs {
  values: "[[9, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[62, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 42
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 43: 42 
Adding parent for node 43: 2 
Adding parent for node 43: 2 42 
Adding node 43
Message: id: 44
name: "detach"
type: COMP_NODE
ctrl_deps: 43
data_deps: 43
inputs {
  values: "[[9, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 43
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 44: 43 
Adding parent for node 44: 43 
Adding parent for node 44: 43 
Adding node 44
Message: id: 45
name: "aten::flatten"
type: COMP_NODE
ctrl_deps: 2
data_deps: 44
inputs {
  values: "[[62, 34, 0, 128, 4, \'cuda:0\'], 0, -1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[41, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 44
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 45: 44 
Adding parent for node 45: 2 
Adding parent for node 45: 2 44 
Adding node 45
Message: id: 46
name: "aten::view"
type: COMP_NODE
ctrl_deps: 45
data_deps: 45
inputs {
  values: "[[62, 34, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 34, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 45
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 46: 45 
Adding parent for node 46: 45 
Adding parent for node 46: 45 
Adding node 46
Message: id: 47
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 46
inputs {
  values: "[[14, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[67, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 46
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 47: 46 
Adding parent for node 47: 2 
Adding parent for node 47: 2 46 
Adding node 47
Message: id: 48
name: "detach"
type: COMP_NODE
ctrl_deps: 47
data_deps: 47
inputs {
  values: "[[14, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 47
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 48: 47 
Adding parent for node 48: 47 
Adding parent for node 48: 47 
Adding node 48
Message: id: 49
name: "aten::flatten"
type: COMP_NODE
ctrl_deps: 2
data_deps: 48
inputs {
  values: "[[67, 42, 0, 16, 4, \'cuda:0\'], 0, -1]"
  shapes: "[[16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[67, 42, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 48
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 49: 48 
Adding parent for node 49: 2 
Adding parent for node 49: 2 48 
Adding node 49
Message: id: 50
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 49
inputs {
  values: "[[22, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[71, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 49
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 50: 49 
Adding parent for node 50: 2 
Adding parent for node 50: 2 49 
Adding node 50
Message: id: 51
name: "detach"
type: COMP_NODE
ctrl_deps: 50
data_deps: 50
inputs {
  values: "[[22, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 50
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 51: 50 
Adding parent for node 51: 50 
Adding parent for node 51: 50 
Adding node 51
Message: id: 52
name: "aten::flatten"
type: COMP_NODE
ctrl_deps: 2
data_deps: 51
inputs {
  values: "[[71, 49, 0, 64, 4, \'cuda:0\'], 0, -1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[74, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 51
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 52: 51 
Adding parent for node 52: 2 
Adding parent for node 52: 2 51 
Adding node 52
Message: id: 53
name: "aten::view"
type: COMP_NODE
ctrl_deps: 52
data_deps: 52
inputs {
  values: "[[71, 49, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[74, 49, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 52
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 53: 52 
Adding parent for node 53: 52 
Adding parent for node 53: 52 
Adding node 53
Message: id: 54
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 53
inputs {
  values: "[[27, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[77, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 53
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 54: 53 
Adding parent for node 54: 2 
Adding parent for node 54: 2 53 
Adding node 54
Message: id: 55
name: "detach"
type: COMP_NODE
ctrl_deps: 54
data_deps: 54
inputs {
  values: "[[27, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 54
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 55: 54 
Adding parent for node 55: 54 
Adding parent for node 55: 54 
Adding node 55
Message: id: 56
name: "aten::flatten"
type: COMP_NODE
ctrl_deps: 2
data_deps: 55
inputs {
  values: "[[77, 56, 0, 4, 4, \'cuda:0\'], 0, -1]"
  shapes: "[[4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[77, 56, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 55
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 56: 55 
Adding parent for node 56: 2 
Adding parent for node 56: 2 55 
Adding node 56
Message: id: 57
name: "aten::cat"
type: COMP_NODE
ctrl_deps: 2
data_deps: 56
inputs {
  values: "[[[41, 34, 0, 128, 4, \'cuda:0\'], [67, 42, 0, 16, 4, \'cuda:0\'], [74, 49, 0, 64, 4, \'cuda:0\'], [77, 56, 0, 4, 4, \'cuda:0\']], 0]"
  shapes: "[[[128], [16], [64], [4]], []]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\', \'Int\']"
}
outputs {
  values: "[[80, 81, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 56
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::cat(Tensor[] tensors, int dim=0) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 57: 56 
Adding parent for node 57: 2 
Adding parent for node 57: 2 56 
Adding node 57
Message: id: 58
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 2
data_deps: 57
inputs {
  values: "[[80, 81, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 57
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 58: 57 
Adding parent for node 58: 2 
Adding parent for node 58: 2 57 
Adding node 58
Message: id: 59
name: "detach"
type: COMP_NODE
ctrl_deps: 58
data_deps: 58
inputs {
  values: "[[80, 81, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 58
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 59: 58 
Adding parent for node 59: 58 
Adding parent for node 59: 58 
Adding node 59
Message: id: 60
name: "aten::split_with_sizes"
type: COMP_NODE
ctrl_deps: 2
data_deps: 59
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [128, 16, 64, 4], 0]"
  shapes: "[[212], [[], [], [], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int,Int,Int]\', \'Int\']"
}
outputs {
  values: "[[[80, 81, 0, 128, 4, \'cuda:0\'], [41, 81, 128, 16, 4, \'cuda:0\'], [67, 81, 144, 64, 4, \'cuda:0\'], [74, 81, 208, 4, 4, \'cuda:0\']]]"
  shapes: "[[[128], [16], [64], [4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 59
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 0
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 60: 59 
Adding parent for node 60: 2 
Adding parent for node 60: 2 59 
Adding node 60
Message: id: 61
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 60
data_deps: 60
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [128], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[80, 81, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 60
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 61: 60 
Adding parent for node 61: 60 
Adding parent for node 61: 60 
Adding node 61
Message: id: 62
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 60
data_deps: 61
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [16], [1], 128]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[41, 81, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 61
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 62: 61 
Adding parent for node 62: 60 
Adding parent for node 62: 60 61 
Adding node 62
Message: id: 63
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 60
data_deps: 62
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [64], [1], 144]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[67, 81, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 62
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 63: 62 
Adding parent for node 63: 60 
Adding parent for node 63: 60 62 
Adding node 63
Message: id: 64
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 60
data_deps: 63
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [4], [1], 208]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[74, 81, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 63
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 64: 63 
Adding parent for node 64: 60 
Adding parent for node 64: 60 63 
Adding node 64
Message: id: 65
name: "aten::view"
type: COMP_NODE
ctrl_deps: 2
data_deps: 64
inputs {
  values: "[[80, 81, 0, 128, 4, \'cuda:0\'], [16, 8]]"
  shapes: "[[128], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[77, 81, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 64
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 65: 64 
Adding parent for node 65: 2 
Adding parent for node 65: 2 64 
Adding node 65
Message: id: 66
name: "aten::view"
type: COMP_NODE
ctrl_deps: 2
data_deps: 65
inputs {
  values: "[[41, 81, 128, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[91, 81, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 65
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 2
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 66: 65 
Adding parent for node 66: 2 
Adding parent for node 66: 2 65 
Adding node 66
Message: id: 67
name: "aten::view"
type: COMP_NODE
ctrl_deps: 2
data_deps: 66
inputs {
  values: "[[67, 81, 144, 64, 4, \'cuda:0\'], [4, 16]]"
  shapes: "[[64], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[93, 81, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 66
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 3
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 67: 66 
Adding parent for node 67: 2 
Adding parent for node 67: 2 66 
Adding node 67
Message: id: 68
name: "aten::view"
type: COMP_NODE
ctrl_deps: 2
data_deps: 67
inputs {
  values: "[[74, 81, 208, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[95, 81, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 67
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 4
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 68: 67 
Adding parent for node 68: 2 
Adding parent for node 68: 2 67 
Adding node 68
Message: id: 69
name: "aten::flatten"
type: COMP_NODE
ctrl_deps: 2
data_deps: 68
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 0, -1]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 68
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 69: 68 
Adding parent for node 69: 2 
Adding parent for node 69: 2 68 
Adding node 69
Message: id: 70
name: "aten::chunk"
type: COMP_NODE
ctrl_deps: 2
data_deps: 69
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 2, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[74, 81, 0, 106, 4, \'cuda:0\'], [80, 81, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 69
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 70: 69 
Adding parent for node 70: 2 
Adding parent for node 70: 2 69 
Adding node 70
Message: id: 71
name: "aten::split"
type: COMP_NODE
ctrl_deps: 70
data_deps: 70
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 106, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[74, 81, 0, 106, 4, \'cuda:0\'], [80, 81, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 70
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 71: 70 
Adding parent for node 71: 70 
Adding parent for node 71: 70 
Adding node 71
Message: id: 72
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 71
data_deps: 71
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 0, 0, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[74, 81, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 71
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 72: 71 
Adding parent for node 72: 71 
Adding parent for node 72: 71 
Adding node 72
Message: id: 73
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 72
data_deps: 72
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 0, 0, 106, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[74, 81, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 72
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 73: 72 
Adding parent for node 73: 72 
Adding parent for node 73: 72 
Adding node 73
Message: id: 74
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 73
data_deps: 73
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [106], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[74, 81, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 73
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 74: 73 
Adding parent for node 74: 73 
Adding parent for node 74: 73 
Adding node 74
Message: id: 75
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 71
data_deps: 74
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 0, 106, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[80, 81, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 74
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 75: 74 
Adding parent for node 75: 71 
Adding parent for node 75: 71 74 
Adding node 75
Message: id: 76
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 75
data_deps: 75
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], 0, 106, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[80, 81, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 75
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 76: 75 
Adding parent for node 76: 75 
Adding parent for node 76: 75 
Adding node 76
Message: id: 77
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 76
data_deps: 76
inputs {
  values: "[[62, 81, 0, 212, 4, \'cuda:0\'], [106], [1], 106]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[80, 81, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 76
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 77: 76 
Adding parent for node 77: 76 
Adding parent for node 77: 76 
Adding node 77
Message: id: 78
name: "aten::clone"
type: COMP_NODE
ctrl_deps: 2
data_deps: 77
inputs {
  values: "[[74, 81, 0, 106, 4, \'cuda:0\'], \'<None>\']"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'None\']"
}
outputs {
  values: "[[80, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 77
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 78: 77 
Adding parent for node 78: 2 
Adding parent for node 78: 2 77 
Adding node 78
Message: id: 79
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 78
data_deps: 78
inputs {
  values: "[[106], [1], 6, 0, \'cuda:0\', \'<None>\']"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'None\']"
}
outputs {
  values: "[[80, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 78
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 79: 78 
Adding parent for node 79: 78 
Adding parent for node 79: 78 
Adding node 79
Message: id: 80
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 78
data_deps: 79
inputs {
  values: "[[80, 107, 0, 106, 4, \'cuda:0\'], [74, 81, 0, 106, 4, \'cuda:0\'], False]"
  shapes: "[[106], [106], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[80, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 79
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 80: 79 
Adding parent for node 80: 78 
Adding parent for node 80: 78 79 
Adding node 80
Message: id: 81
name: "aten::set_"
type: COMP_NODE
ctrl_deps: 2
data_deps: 80
inputs {
  values: "[[62, 110, 0, 212, 4, \'cuda:0\'], [80, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 80
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 81: 80 
Adding parent for node 81: 2 
Adding parent for node 81: 2 80 
Adding node 81
Message: id: 82
name: "aten::set_"
type: COMP_NODE
ctrl_deps: 81
data_deps: 81
inputs {
  values: "[[62, 110, 0, 212, 4, \'cuda:0\'], \'<Storage>\', 0, [106], [1]]"
  shapes: "[[212], [], [], [[]], [[]]]"
  types: "[\'Tensor(float)\', \'Storage\', \'Int\', \'GenericList[Int]\', \'GenericList[Int]\']"
}
outputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 81
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 82: 81 
Adding parent for node 82: 81 
Adding parent for node 82: 81 
Adding node 82
Message: id: 83
name: "aten::randn"
type: COMP_NODE
ctrl_deps: 2
data_deps: 82
inputs {
  values: "[[32, 8], \'<None>\', \'<None>\', \'cpu\', False]"
  shapes: "[[[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 82
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 83: 82 
Adding parent for node 83: 2 
Adding parent for node 83: 2 82 
Adding node 83
Message: id: 84
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 83
data_deps: 83
inputs {
  values: "[[32, 8], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 83
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 84: 83 
Adding parent for node 84: 83 
Adding parent for node 84: 83 
Adding node 84
Message: id: 85
name: "aten::normal_"
type: COMP_NODE
ctrl_deps: 83
data_deps: 84
inputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\'], 0.0, 1.0, \'<None>\']"
  shapes: "[[32, 8], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 84
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::normal_(Tensor(a!) self, float mean=0., float std=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 85: 84 
Adding parent for node 85: 83 
Adding parent for node 85: 83 84 
Adding node 85
Message: id: 86
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 85
inputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[32, 8], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 85
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 86: 85 
Adding parent for node 86: 2 
Adding parent for node 86: 2 85 
Adding node 86
Message: id: 87
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 86
data_deps: 86
inputs {
  values: "[[22, 114, 0, 256, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[32, 8], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 86
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 87: 86 
Adding parent for node 87: 86 
Adding parent for node 87: 86 
Adding node 87
Message: id: 88
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 87
data_deps: 87
inputs {
  values: "[[32, 8], [8, 1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[], []], [[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 87
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 88: 87 
Adding parent for node 88: 87 
Adding parent for node 88: 87 
Adding node 88
Message: id: 89
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 87
data_deps: 88
inputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\'], [22, 114, 0, 256, 4, \'cpu\'], False]"
  shapes: "[[32, 8], [32, 8], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[32, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 88
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 89: 88 
Adding parent for node 89: 87 
Adding parent for node 89: 87 88 
Adding node 89
Message: id: 90
name: "aten::randn"
type: COMP_NODE
ctrl_deps: 2
data_deps: 89
inputs {
  values: "[[32, 4], \'<None>\', \'<None>\', \'cpu\', False]"
  shapes: "[[[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 89
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 90: 89 
Adding parent for node 90: 2 
Adding parent for node 90: 2 89 
Adding node 90
Message: id: 91
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 90
data_deps: 90
inputs {
  values: "[[32, 4], \'<None>\', \'<None>\', \'cpu\', False, \'<None>\']"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'None\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 90
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 91: 90 
Adding parent for node 91: 90 
Adding parent for node 91: 90 
Adding node 91
Message: id: 92
name: "aten::normal_"
type: COMP_NODE
ctrl_deps: 90
data_deps: 91
inputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\'], 0.0, 1.0, \'<None>\']"
  shapes: "[[32, 4], [], [], []]"
  types: "[\'Tensor(float)\', \'Double\', \'Double\', \'None\']"
}
outputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 91
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::normal_(Tensor(a!) self, float mean=0., float std=1., *, Generator? generator=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 92: 91 
Adding parent for node 92: 90 
Adding parent for node 92: 90 91 
Adding node 92
Message: id: 93
name: "aten::to"
type: COMP_NODE
ctrl_deps: 2
data_deps: 92
inputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, False, \'<None>\']"
  shapes: "[[32, 4], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'Bool\', \'None\']"
}
outputs {
  values: "[[74, 49, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 92
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 93: 92 
Adding parent for node 93: 2 
Adding parent for node 93: 2 92 
Adding node 93
Message: id: 94
name: "aten::_to_copy"
type: COMP_NODE
ctrl_deps: 93
data_deps: 93
inputs {
  values: "[[22, 122, 0, 128, 4, \'cpu\'], 6, 0, \'cuda:0\', \'<None>\', False, \'<None>\']"
  shapes: "[[32, 4], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[74, 49, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 93
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 94: 93 
Adding parent for node 94: 93 
Adding parent for node 94: 93 
Adding node 94
Message: id: 95
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 94
data_deps: 94
inputs {
  values: "[[32, 4], [4, 1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[], []], [[], []], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[74, 49, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 94
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 95: 94 
Adding parent for node 95: 94 
Adding parent for node 95: 94 
Adding node 95
Message: id: 96
name: "aten::copy_"
type: COMP_NODE
ctrl_deps: 94
data_deps: 95
inputs {
  values: "[[74, 49, 0, 128, 4, \'cuda:0\'], [22, 122, 0, 128, 4, \'cpu\'], False]"
  shapes: "[[32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Bool\']"
}
outputs {
  values: "[[74, 49, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 95
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 96: 95 
Adding parent for node 96: 94 
Adding parent for node 96: 94 95 
Adding node 96
Message: id: 97
name: "Optimizer.zero_grad#SGD.zero_grad"
type: COMP_NODE
ctrl_deps: 2
data_deps: 96
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 96
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 97: 96 
Adding parent for node 97: 2 
Adding parent for node 97: 2 96 
Adding node 97
Message: id: 98
name: "FullyShardedDataParallel.forward"
type: COMP_NODE
ctrl_deps: 2
data_deps: 97
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 97
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 98: 97 
Adding parent for node 98: 2 
Adding parent for node 98: 2 97 
Adding node 98
Message: id: 99
name: "FullyShardedDataParallel._root_pre_forward"
type: COMP_NODE
ctrl_deps: 98
data_deps: 98
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 98
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 99: 98 
Adding parent for node 99: 98 
Adding parent for node 99: 98 
Adding node 99
Message: id: 100
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 99
data_deps: 99
inputs {
  values: "[[212], 6, \'<None>\', \'cuda:0\', False, \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'Bool\', \'None\']"
}
outputs {
  values: "[[27, 56, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 99
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 100: 99 
Adding parent for node 100: 99 
Adding parent for node 100: 99 
Adding node 100
Message: id: 101
name: "FullyShardedDataParallel._to_kwargs"
type: COMP_NODE
ctrl_deps: 99
data_deps: 100
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 100
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 101: 100 
Adding parent for node 101: 99 
Adding parent for node 101: 99 100 
Adding node 101
Message: id: 102
name: "FullyShardedDataParallel._pre_forward"
type: COMP_NODE
ctrl_deps: 98
data_deps: 101
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 101
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 102: 101 
Adding parent for node 102: 98 
Adding parent for node 102: 98 101 
Adding node 102
Message: id: 103
name: "c10d::_allgather_base_"
type: COMP_NODE
ctrl_deps: 102
data_deps: 102
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [41, 107, 0, 106, 4, \'cuda:0\'], \'<Object>\', False, -1]"
  shapes: "[[212], [106], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 102
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_allgather_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 103: 102 
Adding parent for node 103: 102 
Adding parent for node 103: 102 
Adding node 103
Message: id: 104
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 103
data_deps: 103
inputs {
  values: "[[41, 107, 0, 106, 4, \'cuda:0\'], [1, False], [\'0\', \'default_pg\'], 0, \'_allgather_base\', [], [], 0, 1, 2]"
  shapes: "[[106], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 103
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 104: 103 
Adding parent for node 104: 103 
Adding parent for node 104: 103 
Adding node 104
Message: id: 105
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 104
data_deps: 104
inputs {
  values: "[[0, False], [\'0\', \'default_pg\'], 0, \'init\', [], [], 0, 1, 2]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 104
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 105: 104 
Adding parent for node 105: 104 
Adding parent for node 105: 104 
Adding node 105
Message: id: 106
name: "nccl:_all_gather_base"
type: COMP_NODE
ctrl_deps: 104
data_deps: 105
inputs {
  values: "[[41, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 105
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 106: 105 
Adding parent for node 106: 104 
Adding parent for node 106: 104 105 
Adding node 106
Message: id: 107
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 102
data_deps: 106
inputs {
  values: "[[1, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 106
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 107: 106 
Adding parent for node 107: 102 
Adding parent for node 107: 102 106 
Adding node 107
Message: id: 108
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 102
data_deps: 107
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 107
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 108: 107 
Adding parent for node 108: 102 
Adding parent for node 108: 102 107 
Adding node 108
Message: id: 109
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 108
data_deps: 108
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 108
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 109: 108 
Adding parent for node 109: 108 
Adding parent for node 109: 108 
Adding node 109
Message: id: 110
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 102
data_deps: 109
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 109
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 110: 109 
Adding parent for node 110: 102 
Adding parent for node 110: 102 109 
Adding node 110
Message: id: 111
name: "aten::split_with_sizes"
type: COMP_NODE
ctrl_deps: 102
data_deps: 110
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128, 16, 64, 4], 0]"
  shapes: "[[212], [[], [], [], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int,Int,Int]\', \'Int\']"
}
outputs {
  values: "[[[145, 135, 0, 128, 4, \'cuda:0\'], [147, 135, 128, 16, 4, \'cuda:0\'], [67, 135, 144, 64, 4, \'cuda:0\'], [150, 135, 208, 4, 4, \'cuda:0\']]]"
  shapes: "[[[128], [16], [64], [4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 110
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 111: 110 
Adding parent for node 111: 102 
Adding parent for node 111: 102 110 
Adding node 111
Message: id: 112
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 111
data_deps: 111
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[145, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 111
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 112: 111 
Adding parent for node 112: 111 
Adding parent for node 112: 111 
Adding node 112
Message: id: 113
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 111
data_deps: 112
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [16], [1], 128]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[147, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 112
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 113: 112 
Adding parent for node 113: 111 
Adding parent for node 113: 111 112 
Adding node 113
Message: id: 114
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 111
data_deps: 113
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [64], [1], 144]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[67, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 113
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 114: 113 
Adding parent for node 114: 111 
Adding parent for node 114: 111 113 
Adding node 114
Message: id: 115
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 111
data_deps: 114
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [4], [1], 208]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[150, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 114
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 115: 114 
Adding parent for node 115: 111 
Adding parent for node 115: 111 114 
Adding node 115
Message: id: 116
name: "aten::view"
type: COMP_NODE
ctrl_deps: 102
data_deps: 115
inputs {
  values: "[[145, 135, 0, 128, 4, \'cuda:0\'], [16, 8]]"
  shapes: "[[128], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[152, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 115
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 6
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 116: 115 
Adding parent for node 116: 102 
Adding parent for node 116: 102 115 
Adding node 116
Message: id: 117
name: "aten::view"
type: COMP_NODE
ctrl_deps: 102
data_deps: 116
inputs {
  values: "[[147, 135, 128, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[77, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 116
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 7
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 117: 116 
Adding parent for node 117: 102 
Adding parent for node 117: 102 116 
Adding node 117
Message: id: 118
name: "aten::view"
type: COMP_NODE
ctrl_deps: 102
data_deps: 117
inputs {
  values: "[[67, 135, 144, 64, 4, \'cuda:0\'], [4, 16]]"
  shapes: "[[64], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 117
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 8
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 118: 117 
Adding parent for node 118: 102 
Adding parent for node 118: 102 117 
Adding node 118
Message: id: 119
name: "aten::view"
type: COMP_NODE
ctrl_deps: 102
data_deps: 118
inputs {
  values: "[[150, 135, 208, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[93, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 118
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 9
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 119: 118 
Adding parent for node 119: 102 
Adding parent for node 119: 102 118 
Adding node 119
Message: id: 120
name: "FullyShardedDataParallel._pre_forward_prefetch"
type: COMP_NODE
ctrl_deps: 102
data_deps: 119
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 119
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 120: 119 
Adding parent for node 120: 102 
Adding parent for node 120: 102 119 
Adding node 120
Message: id: 121
name: "aten::expand_as"
type: COMP_NODE
ctrl_deps: 102
data_deps: 120
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [62, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 120
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 10
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 121: 120 
Adding parent for node 121: 102 
Adding parent for node 121: 102 120 
Adding node 121
Message: id: 122
name: "aten::expand"
type: COMP_NODE
ctrl_deps: 121
data_deps: 121
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], False]"
  shapes: "[[212], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\']"
}
outputs {
  values: "[[41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 121
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 10
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 122: 121 
Adding parent for node 122: 121 
Adding parent for node 122: 121 
Adding node 122
Message: id: 123
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 122
data_deps: 122
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], [1], \'<None>\']"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'None\']"
}
outputs {
  values: "[[41, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 122
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 123: 122 
Adding parent for node 123: 122 
Adding parent for node 123: 122 
Adding node 123
Message: id: 124
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 98
data_deps: 123
inputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\'], [152, 135, 0, 128, 4, \'cuda:0\'], [77, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[32, 8], [16, 8], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[150, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 123
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 11
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 124: 123 
Adding parent for node 124: 98 
Adding parent for node 124: 98 123 
Adding node 124
Message: id: 125
name: "aten::t"
type: COMP_NODE
ctrl_deps: 124
data_deps: 124
inputs {
  values: "[[152, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[41, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 124
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 11
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 125: 124 
Adding parent for node 125: 124 
Adding parent for node 125: 124 
Adding node 125
Message: id: 126
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 125
data_deps: 125
inputs {
  values: "[[152, 135, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[41, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 125
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 126: 125 
Adding parent for node 126: 125 
Adding parent for node 126: 125 
Adding node 126
Message: id: 127
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 126
data_deps: 126
inputs {
  values: "[[152, 135, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[41, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 126
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 127: 126 
Adding parent for node 127: 126 
Adding parent for node 127: 126 
Adding node 127
Message: id: 128
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 124
data_deps: 127
inputs {
  values: "[[77, 135, 128, 16, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\'], [41, 135, 0, 128, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[16], [32, 8], [8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[150, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 127
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 12
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 128: 127 
Adding parent for node 128: 124 
Adding parent for node 128: 124 127 
Adding node 128
Message: id: 129
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 128
data_deps: 128
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[147, 166, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 128
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 129: 128 
Adding parent for node 129: 128 
Adding parent for node 129: 128 
Adding node 129
Message: id: 130
name: "aten::relu"
type: COMP_NODE
ctrl_deps: 98
data_deps: 129
inputs {
  values: "[[150, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[41, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 129
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 13
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::relu(Tensor self) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 130: 129 
Adding parent for node 130: 98 
Adding parent for node 130: 98 129 
Adding node 130
Message: id: 131
name: "aten::clamp_min"
type: COMP_NODE
ctrl_deps: 130
data_deps: 130
inputs {
  values: "[[150, 167, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[41, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 130
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::clamp_min(Tensor self, Scalar min) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 131: 130 
Adding parent for node 131: 130 
Adding parent for node 131: 130 
Adding node 131
Message: id: 132
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 98
data_deps: 131
inputs {
  values: "[[41, 166, 0, 512, 4, \'cuda:0\'], [91, 135, 144, 64, 4, \'cuda:0\'], [93, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[32, 16], [4, 16], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 131
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 14
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 132: 131 
Adding parent for node 132: 98 
Adding parent for node 132: 98 131 
Adding node 132
Message: id: 133
name: "aten::t"
type: COMP_NODE
ctrl_deps: 132
data_deps: 132
inputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 132
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 14
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 133: 132 
Adding parent for node 133: 132 
Adding parent for node 133: 132 
Adding node 133
Message: id: 134
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 133
data_deps: 133
inputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 133
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 134: 133 
Adding parent for node 134: 133 
Adding parent for node 134: 133 
Adding node 134
Message: id: 135
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 134
data_deps: 134
inputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 134
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 135: 134 
Adding parent for node 135: 134 
Adding parent for node 135: 134 
Adding node 135
Message: id: 136
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 132
data_deps: 135
inputs {
  values: "[[93, 135, 208, 4, 4, \'cuda:0\'], [41, 166, 0, 512, 4, \'cuda:0\'], [150, 135, 144, 64, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[4], [32, 16], [16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 135
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 15
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 136: 135 
Adding parent for node 136: 132 
Adding parent for node 136: 132 135 
Adding node 136
Message: id: 137
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 136
data_deps: 136
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[176, 177, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 136
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 137: 136 
Adding parent for node 137: 136 
Adding parent for node 137: 136 
Adding node 137
Message: id: 138
name: "FullyShardedDataParallel._post_forward"
type: COMP_NODE
ctrl_deps: 98
data_deps: 137
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 137
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 138: 137 
Adding parent for node 138: 98 
Adding parent for node 138: 98 137 
Adding node 138
Message: id: 139
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 138
data_deps: 138
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 138
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 16
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 139: 138 
Adding parent for node 139: 138 
Adding parent for node 139: 138 
Adding node 139
Message: id: 140
name: "aten::broadcast_tensors"
type: COMP_NODE
ctrl_deps: 2
data_deps: 139
inputs {
  values: "[[[178, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
outputs {
  values: "[[[178, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 139
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 16
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 140: 139 
Adding parent for node 140: 2 
Adding parent for node 140: 2 139 
Adding node 140
Message: id: 141
name: "aten::mse_loss"
type: COMP_NODE
ctrl_deps: 2
data_deps: 140
inputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 140
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 16
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 141: 140 
Adding parent for node 141: 2 
Adding parent for node 141: 2 140 
Adding node 141
Message: id: 142
name: "aten::resize_"
type: COMP_NODE
ctrl_deps: 141
data_deps: 141
inputs {
  values: "[[176, 81, 0, 128, 4, \'cuda:0\'], [], \'<None>\']"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'None\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 141
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 142: 141 
Adding parent for node 142: 141 
Adding parent for node 142: 141 
Adding node 142
Message: id: 143
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 141
data_deps: 142
inputs {
  values: "[[32, 4], 6, \'<None>\', \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'None\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[185, 186, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 142
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 143: 142 
Adding parent for node 143: 141 
Adding parent for node 143: 141 142 
Adding node 143
Message: id: 144
name: "aten::mean"
type: COMP_NODE
ctrl_deps: 141
data_deps: 143
inputs {
  values: "[[185, 186, 0, 128, 4, \'cuda:0\'], [], False, \'<None>\', [176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [], [], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'Bool\', \'None\', \'Tensor(float)\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 143
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 144: 143 
Adding parent for node 144: 141 
Adding parent for node 144: 141 143 
Adding node 144
Message: id: 145
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 144
data_deps: 144
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], [1, 1], [0, 0], \'<None>\']"
  shapes: "[[], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[189, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[1, 1]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 144
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 145: 144 
Adding parent for node 145: 144 
Adding parent for node 145: 144 
Adding node 145
Message: id: 146
name: "aten::ones_like"
type: COMP_NODE
ctrl_deps: 2
data_deps: 145
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 145
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 146: 145 
Adding parent for node 146: 2 
Adding parent for node 146: 2 145 
Adding node 146
Message: id: 147
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 146
data_deps: 146
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 146
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 147: 146 
Adding parent for node 147: 146 
Adding parent for node 147: 146 
Adding node 147
Message: id: 148
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 147
data_deps: 147
inputs {
  values: "[[], [], 6, 0, \'cuda:0\', False]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 147
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 148: 147 
Adding parent for node 148: 147 
Adding parent for node 148: 147 
Adding node 148
Message: id: 149
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 146
data_deps: 148
inputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\'], 1.0]"
  shapes: "[[], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 148
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 149: 148 
Adding parent for node 149: 146 
Adding parent for node 149: 146 148 
Adding node 149
Message: id: 150
name: "[pytorch|profiler|execution_trace|thread]"
type: COMP_NODE
ctrl_deps: 0
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 0
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 150: 
Adding parent for node 150: 0 
Adding parent for node 150: 0 
Adding node 150
Message: id: 151
name: "autograd::engine::evaluate_function: MseLossBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 150
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 149
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 151: 150 
Adding parent for node 151: 150 
Adding parent for node 151: 150 
Adding node 151
Message: id: 152
name: "MseLossBackward0"
type: COMP_NODE
ctrl_deps: 151
data_deps: 151
inputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 150
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 16
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 152: 151 
Adding parent for node 152: 151 
Adding parent for node 152: 151 
Adding node 152
Message: id: 153
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 152
data_deps: 152
inputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\'], [178, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[], [32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 151
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 153: 152 
Adding parent for node 153: 152 
Adding parent for node 153: 152 
Adding node 153
Message: id: 154
name: "aten::zeros_like"
type: COMP_NODE
ctrl_deps: 153
data_deps: 153
inputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'None\', \'Int\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 152
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 154: 153 
Adding parent for node 154: 153 
Adding parent for node 154: 153 
Adding node 154
Message: id: 155
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 154
data_deps: 154
inputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\'], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 153
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 155: 154 
Adding parent for node 155: 154 
Adding parent for node 155: 154 
Adding node 155
Message: id: 156
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 155
data_deps: 155
inputs {
  values: "[[32, 4], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 154
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 156: 155 
Adding parent for node 156: 155 
Adding parent for node 156: 155 
Adding node 156
Message: id: 157
name: "aten::zero_"
type: COMP_NODE
ctrl_deps: 154
data_deps: 156
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 155
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zero_(Tensor(a!) self) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 157: 156 
Adding parent for node 157: 154 
Adding parent for node 157: 154 156 
Adding node 157
Message: id: 158
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 157
data_deps: 157
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 4], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 156
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 158: 157 
Adding parent for node 158: 157 
Adding parent for node 158: 157 
Adding node 158
Message: id: 159
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 153
data_deps: 158
inputs {
  values: "[[185, 186, 0, 1, 4, \'cuda:0\'], [178, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1, [201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[], [32, 4], [32, 4], [], [32, 4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Tensor(float)\']"
}
outputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 157
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 159: 158 
Adding parent for node 159: 153 
Adding parent for node 159: 153 158 
Adding node 159
Message: id: 160
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 159
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 158
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 160: 159 
Adding parent for node 160: 150 
Adding parent for node 160: 150 159 
Adding node 160
Message: id: 161
name: "FullyShardedDataParallel._pre_backward_hook"
type: COMP_NODE
ctrl_deps: 160
data_deps: 160
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 159
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 161: 160 
Adding parent for node 161: 160 
Adding parent for node 161: 160 
Adding node 161
Message: id: 162
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 161
data_deps: 161
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[209, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 160
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 162: 161 
Adding parent for node 162: 161 
Adding parent for node 162: 161 
Adding node 162
Message: id: 163
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 162
data_deps: 162
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[209, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 161
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 163: 162 
Adding parent for node 163: 162 
Adding parent for node 163: 162 
Adding node 163
Message: id: 164
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 161
data_deps: 163
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [209, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 162
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 164: 163 
Adding parent for node 164: 161 
Adding parent for node 164: 161 163 
Adding node 164
Message: id: 165
name: "FullyShardedDataParallel._pre_backward_prefetch"
type: COMP_NODE
ctrl_deps: 161
data_deps: 164
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 163
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 165: 164 
Adding parent for node 165: 161 
Adding parent for node 165: 161 164 
Adding node 165
Message: id: 166
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 160
data_deps: 165
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 164
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 15
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 166: 165 
Adding parent for node 166: 160 
Adding parent for node 166: 160 165 
Adding node 166
Message: id: 167
name: "aten::t"
type: COMP_NODE
ctrl_deps: 166
data_deps: 166
inputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[209, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 165
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 167: 166 
Adding parent for node 167: 166 
Adding parent for node 167: 166 
Adding node 167
Message: id: 168
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 167
data_deps: 167
inputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[209, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 166
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 168: 167 
Adding parent for node 168: 167 
Adding parent for node 168: 167 
Adding node 168
Message: id: 169
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 168
data_deps: 168
inputs {
  values: "[[150, 135, 144, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[209, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 167
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 169: 168 
Adding parent for node 169: 168 
Adding parent for node 169: 168 
Adding node 169
Message: id: 170
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 166
data_deps: 169
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\'], [209, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[217, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 168
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 170: 169 
Adding parent for node 170: 166 
Adding parent for node 170: 166 169 
Adding node 170
Message: id: 171
name: "aten::t"
type: COMP_NODE
ctrl_deps: 166
data_deps: 170
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 169
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 171: 170 
Adding parent for node 171: 166 
Adding parent for node 171: 166 170 
Adding node 171
Message: id: 172
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 171
data_deps: 171
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 170
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 172: 171 
Adding parent for node 172: 171 
Adding parent for node 172: 171 
Adding node 172
Message: id: 173
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 172
data_deps: 172
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\'], [4, 32], [1, 4], \'<None>\']"
  shapes: "[[32, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 171
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 173: 172 
Adding parent for node 173: 172 
Adding parent for node 173: 172 
Adding node 173
Message: id: 174
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 166
data_deps: 173
inputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\'], [41, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[4, 32], [32, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[222, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 172
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 174: 173 
Adding parent for node 174: 166 
Adding parent for node 174: 166 173 
Adding node 174
Message: id: 175
name: "aten::t"
type: COMP_NODE
ctrl_deps: 166
data_deps: 174
inputs {
  values: "[[222, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 173
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 175: 174 
Adding parent for node 175: 166 
Adding parent for node 175: 166 174 
Adding node 175
Message: id: 176
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 175
data_deps: 175
inputs {
  values: "[[222, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 174
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 176: 175 
Adding parent for node 176: 175 
Adding parent for node 176: 175 
Adding node 176
Message: id: 177
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 176
data_deps: 176
inputs {
  values: "[[222, 223, 0, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 175
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 177: 176 
Adding parent for node 177: 176 
Adding parent for node 177: 176 
Adding node 177
Message: id: 178
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 160
data_deps: 177
inputs {
  values: "[[201, 167, 0, 128, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 4], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[209, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[1, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 176
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 178: 177 
Adding parent for node 178: 160 
Adding parent for node 178: 160 177 
Adding node 178
Message: id: 179
name: "aten::view"
type: COMP_NODE
ctrl_deps: 160
data_deps: 178
inputs {
  values: "[[209, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[1, 4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 177
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 179: 178 
Adding parent for node 179: 160 
Adding parent for node 179: 160 178 
Adding node 179
Message: id: 180
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 179
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 178
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 180: 179 
Adding parent for node 180: 150 
Adding parent for node 180: 150 179 
Adding node 180
Message: id: 181
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 180
data_deps: 180
inputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 179
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 14
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 181: 180 
Adding parent for node 181: 180 
Adding parent for node 181: 180 
Adding node 181
Message: id: 182
name: "aten::t"
type: COMP_NODE
ctrl_deps: 181
data_deps: 181
inputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 180
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 182: 181 
Adding parent for node 182: 181 
Adding parent for node 182: 181 
Adding node 182
Message: id: 183
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 182
data_deps: 182
inputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 181
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 183: 182 
Adding parent for node 183: 182 
Adding parent for node 183: 182 
Adding node 183
Message: id: 184
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 183
data_deps: 183
inputs {
  values: "[[227, 223, 0, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 182
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 184: 183 
Adding parent for node 184: 183 
Adding parent for node 184: 183 
Adding node 184
Message: id: 185
name: "autograd::engine::evaluate_function: ReluBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 184
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 183
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 185: 184 
Adding parent for node 185: 150 
Adding parent for node 185: 150 184 
Adding node 185
Message: id: 186
name: "ReluBackward0"
type: COMP_NODE
ctrl_deps: 185
data_deps: 185
inputs {
  values: "[[217, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 184
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 13
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 186: 185 
Adding parent for node 186: 185 
Adding parent for node 186: 185 
Adding node 186
Message: id: 187
name: "aten::threshold_backward"
type: COMP_NODE
ctrl_deps: 186
data_deps: 186
inputs {
  values: "[[217, 177, 0, 512, 4, \'cuda:0\'], [227, 166, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], [32, 16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 185
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 187: 186 
Adding parent for node 187: 186 
Adding parent for node 187: 186 
Adding node 187
Message: id: 188
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 187
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 186
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 188: 187 
Adding parent for node 188: 150 
Adding parent for node 188: 150 187 
Adding node 188
Message: id: 189
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 188
data_deps: 188
inputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 187
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 12
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 189: 188 
Adding parent for node 189: 188 
Adding parent for node 189: 188 
Adding node 189
Message: id: 190
name: "aten::t"
type: COMP_NODE
ctrl_deps: 189
data_deps: 189
inputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 188
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 190: 189 
Adding parent for node 190: 189 
Adding parent for node 190: 189 
Adding node 190
Message: id: 191
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 190
data_deps: 190
inputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 189
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 191: 190 
Adding parent for node 191: 190 
Adding parent for node 191: 190 
Adding node 191
Message: id: 192
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 191
data_deps: 191
inputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\'], [16, 32], [1, 16], \'<None>\']"
  shapes: "[[32, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 190
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 192: 191 
Adding parent for node 192: 191 
Adding parent for node 192: 191 
Adding node 192
Message: id: 193
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 189
data_deps: 192
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[16, 32], [32, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[217, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 191
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 193: 192 
Adding parent for node 193: 189 
Adding parent for node 193: 189 192 
Adding node 193
Message: id: 194
name: "aten::t"
type: COMP_NODE
ctrl_deps: 189
data_deps: 193
inputs {
  values: "[[217, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 192
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 194: 193 
Adding parent for node 194: 189 
Adding parent for node 194: 189 193 
Adding node 194
Message: id: 195
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 194
data_deps: 194
inputs {
  values: "[[217, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 193
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 195: 194 
Adding parent for node 195: 194 
Adding parent for node 195: 194 
Adding node 195
Message: id: 196
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 195
data_deps: 195
inputs {
  values: "[[217, 167, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 194
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 196: 195 
Adding parent for node 196: 195 
Adding parent for node 196: 195 
Adding node 196
Message: id: 197
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 188
data_deps: 196
inputs {
  values: "[[41, 240, 0, 512, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 16], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[246, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[1, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 195
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 197: 196 
Adding parent for node 197: 188 
Adding parent for node 197: 188 196 
Adding node 197
Message: id: 198
name: "aten::view"
type: COMP_NODE
ctrl_deps: 188
data_deps: 197
inputs {
  values: "[[246, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[1, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[254, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 196
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 198: 197 
Adding parent for node 198: 188 
Adding parent for node 198: 188 197 
Adding node 198
Message: id: 199
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 198
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 197
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 199: 198 
Adding parent for node 199: 150 
Adding parent for node 199: 150 198 
Adding node 199
Message: id: 200
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 199
data_deps: 199
inputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 198
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 11
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 200: 199 
Adding parent for node 200: 199 
Adding parent for node 200: 199 
Adding node 200
Message: id: 201
name: "aten::t"
type: COMP_NODE
ctrl_deps: 200
data_deps: 200
inputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 199
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 201: 200 
Adding parent for node 201: 200 
Adding parent for node 201: 200 
Adding node 201
Message: id: 202
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 201
data_deps: 201
inputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 200
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 202: 201 
Adding parent for node 202: 201 
Adding parent for node 202: 201 
Adding node 202
Message: id: 203
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 202
data_deps: 202
inputs {
  values: "[[227, 167, 0, 128, 4, \'cuda:0\'], [16, 8], [8, 1], \'<None>\']"
  shapes: "[[8, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 201
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 203: 202 
Adding parent for node 203: 202 
Adding parent for node 203: 202 
Adding node 203
Message: id: 204
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 203
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 202
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 204: 203 
Adding parent for node 204: 150 
Adding parent for node 204: 150 203 
Adding node 204
Message: id: 205
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 204
data_deps: 204
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 203
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 9
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 205: 204 
Adding parent for node 205: 204 
Adding parent for node 205: 204 
Adding node 205
Message: id: 206
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 205
data_deps: 205
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[227, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 204
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 206: 205 
Adding parent for node 206: 205 
Adding parent for node 206: 205 
Adding node 206
Message: id: 207
name: "aten::view"
type: COMP_NODE
ctrl_deps: 206
data_deps: 206
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[227, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 205
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 207: 206 
Adding parent for node 207: 206 
Adding parent for node 207: 206 
Adding node 207
Message: id: 208
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 207
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 206
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 208: 207 
Adding parent for node 208: 150 
Adding parent for node 208: 150 207 
Adding node 208
Message: id: 209
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 208
data_deps: 208
inputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 207
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 8
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 209: 208 
Adding parent for node 209: 208 
Adding parent for node 209: 208 
Adding node 209
Message: id: 210
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 209
data_deps: 209
inputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 208
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 210: 209 
Adding parent for node 210: 209 
Adding parent for node 210: 209 
Adding node 210
Message: id: 211
name: "aten::view"
type: COMP_NODE
ctrl_deps: 210
data_deps: 210
inputs {
  values: "[[150, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 209
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 211: 210 
Adding parent for node 211: 210 
Adding parent for node 211: 210 
Adding node 211
Message: id: 212
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 211
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 210
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 212: 211 
Adding parent for node 212: 150 
Adding parent for node 212: 150 211 
Adding node 212
Message: id: 213
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 212
data_deps: 212
inputs {
  values: "[[254, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 211
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 7
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 213: 212 
Adding parent for node 213: 212 
Adding parent for node 213: 212 
Adding node 213
Message: id: 214
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 213
data_deps: 213
inputs {
  values: "[[254, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[150, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 212
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 214: 213 
Adding parent for node 214: 213 
Adding parent for node 214: 213 
Adding node 214
Message: id: 215
name: "aten::view"
type: COMP_NODE
ctrl_deps: 214
data_deps: 214
inputs {
  values: "[[254, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[150, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 213
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 215: 214 
Adding parent for node 215: 214 
Adding parent for node 215: 214 
Adding node 215
Message: id: 216
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 215
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 214
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 216: 215 
Adding parent for node 216: 150 
Adding parent for node 216: 150 215 
Adding node 216
Message: id: 217
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 216
data_deps: 216
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 215
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 6
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 217: 216 
Adding parent for node 217: 216 
Adding parent for node 217: 216 
Adding node 217
Message: id: 218
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 217
data_deps: 217
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[254, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 216
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 218: 217 
Adding parent for node 218: 217 
Adding parent for node 218: 217 
Adding node 218
Message: id: 219
name: "aten::view"
type: COMP_NODE
ctrl_deps: 218
data_deps: 218
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[254, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 217
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 219: 218 
Adding parent for node 219: 218 
Adding parent for node 219: 218 
Adding node 219
Message: id: 220
name: "autograd::engine::evaluate_function: SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 219
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 218
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 220: 219 
Adding parent for node 220: 150 
Adding parent for node 220: 150 219 
Adding node 220
Message: id: 221
name: "SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 220
data_deps: 220
inputs {
  values: "[[254, 167, 0, 128, 4, \'cuda:0\'], [150, 252, 0, 16, 4, \'cuda:0\'], [231, 223, 0, 64, 4, \'cuda:0\'], [227, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[128], [16], [64], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 219
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: 5
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 221: 220 
Adding parent for node 221: 220 
Adding parent for node 221: 220 
Adding node 221
Message: id: 222
name: "aten::cat"
type: COMP_NODE
ctrl_deps: 221
data_deps: 221
inputs {
  values: "[[[254, 167, 0, 128, 4, \'cuda:0\'], [150, 252, 0, 16, 4, \'cuda:0\'], [231, 223, 0, 64, 4, \'cuda:0\'], [227, 229, 0, 4, 4, \'cuda:0\']], 0]"
  shapes: "[[[128], [16], [64], [4]], []]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\', \'Int\']"
}
outputs {
  values: "[[41, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 220
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::cat(Tensor[] tensors, int dim=0) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 222: 221 
Adding parent for node 222: 221 
Adding parent for node 222: 221 
Adding node 222
Message: id: 223
name: "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 150
data_deps: 222
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 221
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 223: 222 
Adding parent for node 223: 150 
Adding parent for node 223: 150 222 
Adding node 223
Message: id: 224
name: "torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 223
data_deps: 223
inputs {
  values: "[[41, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 222
}
attr {
  name: "fw_parent"
  int64_val: 3
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 224: 223 
Adding parent for node 224: 223 
Adding parent for node 224: 223 
Adding node 224
Message: id: 225
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 224
data_deps: 224
inputs {
  values: "[[41, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 223
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 225: 224 
Adding parent for node 225: 224 
Adding parent for node 225: 224 
Adding node 225
Message: id: 226
name: "detach"
type: COMP_NODE
ctrl_deps: 225
data_deps: 225
inputs {
  values: "[[41, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 224
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 226: 225 
Adding parent for node 226: 225 
Adding parent for node 226: 225 
Adding node 226
Message: id: 227
name: "FullyShardedDataParallel._post_backward_hook"
type: COMP_NODE
ctrl_deps: 223
data_deps: 226
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 225
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 227: 226 
Adding parent for node 227: 223 
Adding parent for node 227: 223 226 
Adding node 227
Message: id: 228
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 227
data_deps: 227
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 226
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 228: 227 
Adding parent for node 228: 227 
Adding parent for node 228: 227 
Adding node 228
Message: id: 229
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 227
data_deps: 228
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 227
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 229: 228 
Adding parent for node 229: 227 
Adding parent for node 229: 227 228 
Adding node 229
Message: id: 230
name: "FullyShardedDataParallel._post_backward_prefetch"
type: COMP_NODE
ctrl_deps: 227
data_deps: 229
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 228
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 230: 229 
Adding parent for node 230: 227 
Adding parent for node 230: 227 229 
Adding node 230
Message: id: 231
name: "aten::chunk"
type: COMP_NODE
ctrl_deps: 227
data_deps: 230
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 2, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[231, 279, 0, 106, 4, \'cuda:0\'], [150, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 229
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 231: 230 
Adding parent for node 231: 227 
Adding parent for node 231: 227 230 
Adding node 231
Message: id: 232
name: "aten::split"
type: COMP_NODE
ctrl_deps: 231
data_deps: 231
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 106, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[231, 279, 0, 106, 4, \'cuda:0\'], [150, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 230
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 232: 231 
Adding parent for node 232: 231 
Adding parent for node 232: 231 
Adding node 232
Message: id: 233
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 232
data_deps: 232
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[231, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 231
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 233: 232 
Adding parent for node 233: 232 
Adding parent for node 233: 232 
Adding node 233
Message: id: 234
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 233
data_deps: 233
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[231, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 232
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 234: 233 
Adding parent for node 234: 233 
Adding parent for node 234: 233 
Adding node 234
Message: id: 235
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 234
data_deps: 234
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[231, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 233
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 235: 234 
Adding parent for node 235: 234 
Adding parent for node 235: 234 
Adding node 235
Message: id: 236
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 232
data_deps: 235
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[150, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 234
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 236: 235 
Adding parent for node 236: 232 
Adding parent for node 236: 232 235 
Adding node 236
Message: id: 237
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 236
data_deps: 236
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[150, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 235
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 237: 236 
Adding parent for node 237: 236 
Adding parent for node 237: 236 
Adding node 237
Message: id: 238
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 237
data_deps: 237
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 106]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[150, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 236
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 238: 237 
Adding parent for node 238: 237 
Adding parent for node 238: 237 
Adding node 238
Message: id: 239
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 227
data_deps: 238
inputs {
  values: "[[231, 279, 0, 106, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, \'<None>\']"
  shapes: "[[106], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[246, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 237
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 239: 238 
Adding parent for node 239: 227 
Adding parent for node 239: 227 238 
Adding node 239
Message: id: 240
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 239
data_deps: 239
inputs {
  values: "[[106], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[246, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 238
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 240: 239 
Adding parent for node 240: 239 
Adding parent for node 240: 239 
Adding node 240
Message: id: 241
name: "aten::div_"
type: COMP_NODE
ctrl_deps: 227
data_deps: 240
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], [231, 300, 0, 1, 8, \'cpu\']]"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Tensor(double)\']"
}
outputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 239
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 241: 240 
Adding parent for node 241: 227 
Adding parent for node 241: 227 240 
Adding node 241
Message: id: 242
name: "c10d::_reduce_scatter_base_"
type: COMP_NODE
ctrl_deps: 227
data_deps: 241
inputs {
  values: "[[246, 298, 0, 106, 4, \'cuda:0\'], [222, 279, 0, 212, 4, \'cuda:0\'], \'<Object>\', \'<Object>\', False, -1]"
  shapes: "[[106], [212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[246, 298, 0, 106, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 240
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_reduce_scatter_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, __torch__.torch.classes.c10d.ReduceOp reduce_op, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 242: 241 
Adding parent for node 242: 227 
Adding parent for node 242: 227 241 
Adding node 242
Message: id: 243
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 242
data_deps: 242
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], [2, False], [\'0\', \'default_pg\'], 0, \'_reduce_scatter_base\', [], [], 0, 1, 2]"
  shapes: "[[212], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[246, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 241
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 243: 242 
Adding parent for node 243: 242 
Adding parent for node 243: 242 
Adding node 243
Message: id: 244
name: "nccl:_reduce_scatter_base"
type: COMP_NODE
ctrl_deps: 243
data_deps: 243
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 242
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 244: 243 
Adding parent for node 244: 243 
Adding parent for node 244: 243 
Adding node 244
Message: id: 245
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 227
data_deps: 244
inputs {
  values: "[[2, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 243
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 245: 244 
Adding parent for node 245: 227 
Adding parent for node 245: 227 244 
Adding node 245
Message: id: 246
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 227
data_deps: 245
inputs {
  values: "[[209, 279, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 244
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 246: 245 
Adding parent for node 246: 227 
Adding parent for node 246: 227 245 
Adding node 246
Message: id: 247
name: "Optimizer.step#SGD.step"
type: COMP_NODE
ctrl_deps: 2
data_deps: 149
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 245
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 247: 149 
Adding parent for node 247: 2 
Adding parent for node 247: 2 149 
Adding node 247
Message: id: 248
name: "aten::_foreach_add_"
type: COMP_NODE
ctrl_deps: 247
data_deps: 247
inputs {
  values: "[[[62, 107, 0, 106, 4, \'cuda:0\']], [[246, 298, 0, 106, 4, \'cuda:0\']], -0.01]"
  shapes: "[[[106]], [[106]], []]"
  types: "[\'GenericList[Tensor(float)]\', \'GenericList[Tensor(float)]\', \'Double\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 246
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 248: 247 
Adding parent for node 248: 247 
Adding parent for node 248: 247 
Adding node 248
Message: id: 249
name: "aten::result_type"
type: COMP_NODE
ctrl_deps: 248
data_deps: 248
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], -0.01]"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[6]"
  shapes: "[[]]"
  types: "[\'Int\']"
}
attr {
  name: "rf_id"
  int64_val: 247
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 249: 248 
Adding parent for node 249: 248 
Adding parent for node 249: 248 
Adding node 249
Message: id: 250
name: "aten::item"
type: COMP_NODE
ctrl_deps: 2
data_deps: 249
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.072972]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 248
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::item(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 250: 249 
Adding parent for node 250: 2 
Adding parent for node 250: 2 249 
Adding node 250
Message: id: 251
name: "aten::_local_scalar_dense"
type: COMP_NODE
ctrl_deps: 250
data_deps: 250
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.072972]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 249
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_local_scalar_dense(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 251: 250 
Adding parent for node 251: 250 
Adding parent for node 251: 250 
Adding node 251
Message: id: 252
name: "ProfilerStep#1"
type: COMP_NODE
ctrl_deps: 1
data_deps: 251
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 250
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 252: 251 
Adding parent for node 252: 1 
Adding parent for node 252: 1 251 
Adding node 252
Message: id: 253
name: "Optimizer.zero_grad#SGD.zero_grad"
type: COMP_NODE
ctrl_deps: 252
data_deps: 252
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 251
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 253: 252 
Adding parent for node 253: 252 
Adding parent for node 253: 252 
Adding node 253
Message: id: 254
name: "FullyShardedDataParallel.forward"
type: COMP_NODE
ctrl_deps: 252
data_deps: 253
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 252
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 254: 253 
Adding parent for node 254: 252 
Adding parent for node 254: 252 253 
Adding node 254
Message: id: 255
name: "FullyShardedDataParallel._root_pre_forward"
type: COMP_NODE
ctrl_deps: 254
data_deps: 254
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 253
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 255: 254 
Adding parent for node 255: 254 
Adding parent for node 255: 254 
Adding node 255
Message: id: 256
name: "FullyShardedDataParallel._to_kwargs"
type: COMP_NODE
ctrl_deps: 255
data_deps: 255
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 254
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 256: 255 
Adding parent for node 256: 255 
Adding parent for node 256: 255 
Adding node 256
Message: id: 257
name: "FullyShardedDataParallel._pre_forward"
type: COMP_NODE
ctrl_deps: 254
data_deps: 256
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 255
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 257: 256 
Adding parent for node 257: 254 
Adding parent for node 257: 254 256 
Adding node 257
Message: id: 258
name: "c10d::_allgather_base_"
type: COMP_NODE
ctrl_deps: 257
data_deps: 257
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [246, 107, 0, 106, 4, \'cuda:0\'], \'<Object>\', False, -1]"
  shapes: "[[212], [106], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 256
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_allgather_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 258: 257 
Adding parent for node 258: 257 
Adding parent for node 258: 257 
Adding node 258
Message: id: 259
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 258
data_deps: 258
inputs {
  values: "[[246, 107, 0, 106, 4, \'cuda:0\'], [3, False], [\'0\', \'default_pg\'], 0, \'_allgather_base\', [], [], 0, 1, 2]"
  shapes: "[[106], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 257
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 259: 258 
Adding parent for node 259: 258 
Adding parent for node 259: 258 
Adding node 259
Message: id: 260
name: "nccl:_all_gather_base"
type: COMP_NODE
ctrl_deps: 259
data_deps: 259
inputs {
  values: "[[246, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 258
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 260: 259 
Adding parent for node 260: 259 
Adding parent for node 260: 259 
Adding node 260
Message: id: 261
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 257
data_deps: 260
inputs {
  values: "[[3, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 259
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 261: 260 
Adding parent for node 261: 257 
Adding parent for node 261: 257 260 
Adding node 261
Message: id: 262
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 257
data_deps: 261
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 260
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 262: 261 
Adding parent for node 262: 257 
Adding parent for node 262: 257 261 
Adding node 262
Message: id: 263
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 262
data_deps: 262
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 261
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 263: 262 
Adding parent for node 263: 262 
Adding parent for node 263: 262 
Adding node 263
Message: id: 264
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 257
data_deps: 263
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 262
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 264: 263 
Adding parent for node 264: 257 
Adding parent for node 264: 257 263 
Adding node 264
Message: id: 265
name: "aten::split_with_sizes"
type: COMP_NODE
ctrl_deps: 257
data_deps: 264
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128, 16, 64, 4], 0]"
  shapes: "[[212], [[], [], [], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int,Int,Int]\', \'Int\']"
}
outputs {
  values: "[[[326, 135, 0, 128, 4, \'cuda:0\'], [328, 135, 128, 16, 4, \'cuda:0\'], [330, 135, 144, 64, 4, \'cuda:0\'], [332, 135, 208, 4, 4, \'cuda:0\']]]"
  shapes: "[[[128], [16], [64], [4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 263
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 265: 264 
Adding parent for node 265: 257 
Adding parent for node 265: 257 264 
Adding node 265
Message: id: 266
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 265
data_deps: 265
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[326, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 264
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 266: 265 
Adding parent for node 266: 265 
Adding parent for node 266: 265 
Adding node 266
Message: id: 267
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 265
data_deps: 266
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [16], [1], 128]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[328, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 265
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 267: 266 
Adding parent for node 267: 265 
Adding parent for node 267: 265 266 
Adding node 267
Message: id: 268
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 265
data_deps: 267
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [64], [1], 144]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[330, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 266
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 268: 267 
Adding parent for node 268: 265 
Adding parent for node 268: 265 267 
Adding node 268
Message: id: 269
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 265
data_deps: 268
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [4], [1], 208]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[332, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 267
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 269: 268 
Adding parent for node 269: 265 
Adding parent for node 269: 265 268 
Adding node 269
Message: id: 270
name: "aten::view"
type: COMP_NODE
ctrl_deps: 257
data_deps: 269
inputs {
  values: "[[326, 135, 0, 128, 4, \'cuda:0\'], [16, 8]]"
  shapes: "[[128], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[334, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 268
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 18
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 270: 269 
Adding parent for node 270: 257 
Adding parent for node 270: 257 269 
Adding node 270
Message: id: 271
name: "aten::view"
type: COMP_NODE
ctrl_deps: 257
data_deps: 270
inputs {
  values: "[[328, 135, 128, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[152, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 269
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 19
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 271: 270 
Adding parent for node 271: 257 
Adding parent for node 271: 257 270 
Adding node 271
Message: id: 272
name: "aten::view"
type: COMP_NODE
ctrl_deps: 257
data_deps: 271
inputs {
  values: "[[330, 135, 144, 64, 4, \'cuda:0\'], [4, 16]]"
  shapes: "[[64], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[77, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 270
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 20
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 272: 271 
Adding parent for node 272: 257 
Adding parent for node 272: 257 271 
Adding node 272
Message: id: 273
name: "aten::view"
type: COMP_NODE
ctrl_deps: 257
data_deps: 272
inputs {
  values: "[[332, 135, 208, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[91, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 271
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 21
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 273: 272 
Adding parent for node 273: 257 
Adding parent for node 273: 257 272 
Adding node 273
Message: id: 274
name: "FullyShardedDataParallel._pre_forward_prefetch"
type: COMP_NODE
ctrl_deps: 257
data_deps: 273
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 272
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 274: 273 
Adding parent for node 274: 257 
Adding parent for node 274: 257 273 
Adding node 274
Message: id: 275
name: "aten::expand_as"
type: COMP_NODE
ctrl_deps: 257
data_deps: 274
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [62, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 273
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 22
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 275: 274 
Adding parent for node 275: 257 
Adding parent for node 275: 257 274 
Adding node 275
Message: id: 276
name: "aten::expand"
type: COMP_NODE
ctrl_deps: 275
data_deps: 275
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], False]"
  shapes: "[[212], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\']"
}
outputs {
  values: "[[246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 274
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 22
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 276: 275 
Adding parent for node 276: 275 
Adding parent for node 276: 275 
Adding node 276
Message: id: 277
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 276
data_deps: 276
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], [1], \'<None>\']"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'None\']"
}
outputs {
  values: "[[246, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 275
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 277: 276 
Adding parent for node 277: 276 
Adding parent for node 277: 276 
Adding node 277
Message: id: 278
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 254
data_deps: 277
inputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\'], [334, 135, 0, 128, 4, \'cuda:0\'], [152, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[32, 8], [16, 8], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[332, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 276
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 23
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 278: 277 
Adding parent for node 278: 254 
Adding parent for node 278: 254 277 
Adding node 278
Message: id: 279
name: "aten::t"
type: COMP_NODE
ctrl_deps: 278
data_deps: 278
inputs {
  values: "[[334, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[246, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 277
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 23
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 279: 278 
Adding parent for node 279: 278 
Adding parent for node 279: 278 
Adding node 279
Message: id: 280
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 279
data_deps: 279
inputs {
  values: "[[334, 135, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[246, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 278
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 280: 279 
Adding parent for node 280: 279 
Adding parent for node 280: 279 
Adding node 280
Message: id: 281
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 280
data_deps: 280
inputs {
  values: "[[334, 135, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[246, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 279
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 281: 280 
Adding parent for node 281: 280 
Adding parent for node 281: 280 
Adding node 281
Message: id: 282
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 278
data_deps: 281
inputs {
  values: "[[152, 135, 128, 16, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\'], [246, 135, 0, 128, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[16], [32, 8], [8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 280
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 24
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 282: 281 
Adding parent for node 282: 278 
Adding parent for node 282: 278 281 
Adding node 282
Message: id: 283
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 282
data_deps: 282
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[326, 166, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 281
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 283: 282 
Adding parent for node 283: 282 
Adding parent for node 283: 282 
Adding node 283
Message: id: 284
name: "aten::relu"
type: COMP_NODE
ctrl_deps: 254
data_deps: 283
inputs {
  values: "[[332, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[246, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 282
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 25
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::relu(Tensor self) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 284: 283 
Adding parent for node 284: 254 
Adding parent for node 284: 254 283 
Adding node 284
Message: id: 285
name: "aten::clamp_min"
type: COMP_NODE
ctrl_deps: 284
data_deps: 284
inputs {
  values: "[[332, 167, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[246, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 283
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::clamp_min(Tensor self, Scalar min) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 285: 284 
Adding parent for node 285: 284 
Adding parent for node 285: 284 
Adding node 285
Message: id: 286
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 254
data_deps: 285
inputs {
  values: "[[246, 166, 0, 512, 4, \'cuda:0\'], [77, 135, 144, 64, 4, \'cuda:0\'], [91, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[32, 16], [4, 16], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 284
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 26
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 286: 285 
Adding parent for node 286: 254 
Adding parent for node 286: 254 285 
Adding node 286
Message: id: 287
name: "aten::t"
type: COMP_NODE
ctrl_deps: 286
data_deps: 286
inputs {
  values: "[[77, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 285
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 26
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 287: 286 
Adding parent for node 287: 286 
Adding parent for node 287: 286 
Adding node 287
Message: id: 288
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 287
data_deps: 287
inputs {
  values: "[[77, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 286
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 288: 287 
Adding parent for node 288: 287 
Adding parent for node 288: 287 
Adding node 288
Message: id: 289
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 288
data_deps: 288
inputs {
  values: "[[77, 135, 144, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 287
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 289: 288 
Adding parent for node 289: 288 
Adding parent for node 289: 288 
Adding node 289
Message: id: 290
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 286
data_deps: 289
inputs {
  values: "[[91, 135, 208, 4, 4, \'cuda:0\'], [246, 166, 0, 512, 4, \'cuda:0\'], [332, 135, 144, 64, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[4], [32, 16], [16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 288
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 27
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 290: 289 
Adding parent for node 290: 286 
Adding parent for node 290: 286 289 
Adding node 290
Message: id: 291
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 290
data_deps: 290
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[93, 177, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 289
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 291: 290 
Adding parent for node 291: 290 
Adding parent for node 291: 290 
Adding node 291
Message: id: 292
name: "FullyShardedDataParallel._post_forward"
type: COMP_NODE
ctrl_deps: 254
data_deps: 291
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 290
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 292: 291 
Adding parent for node 292: 254 
Adding parent for node 292: 254 291 
Adding node 292
Message: id: 293
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 292
data_deps: 292
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 291
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 28
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 293: 292 
Adding parent for node 293: 292 
Adding parent for node 293: 292 
Adding node 293
Message: id: 294
name: "aten::broadcast_tensors"
type: COMP_NODE
ctrl_deps: 252
data_deps: 293
inputs {
  values: "[[[356, 186, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
outputs {
  values: "[[[356, 186, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 292
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 28
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 294: 293 
Adding parent for node 294: 252 
Adding parent for node 294: 252 293 
Adding node 294
Message: id: 295
name: "aten::mse_loss"
type: COMP_NODE
ctrl_deps: 252
data_deps: 294
inputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 293
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 28
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 295: 294 
Adding parent for node 295: 252 
Adding parent for node 295: 252 294 
Adding node 295
Message: id: 296
name: "aten::resize_"
type: COMP_NODE
ctrl_deps: 295
data_deps: 295
inputs {
  values: "[[178, 56, 0, 128, 4, \'cuda:0\'], [], \'<None>\']"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'None\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 294
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 296: 295 
Adding parent for node 296: 295 
Adding parent for node 296: 295 
Adding node 296
Message: id: 297
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 295
data_deps: 296
inputs {
  values: "[[32, 4], 6, \'<None>\', \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'None\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 295
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 297: 296 
Adding parent for node 297: 295 
Adding parent for node 297: 295 296 
Adding node 297
Message: id: 298
name: "aten::mean"
type: COMP_NODE
ctrl_deps: 295
data_deps: 297
inputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\'], [], False, \'<None>\', [178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [], [], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'Bool\', \'None\', \'Tensor(float)\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 296
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 298: 297 
Adding parent for node 298: 295 
Adding parent for node 298: 295 297 
Adding node 298
Message: id: 299
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 298
data_deps: 298
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], [1, 1], [0, 0], \'<None>\']"
  shapes: "[[], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[365, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[1, 1]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 297
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 299: 298 
Adding parent for node 299: 298 
Adding parent for node 299: 298 
Adding node 299
Message: id: 300
name: "aten::ones_like"
type: COMP_NODE
ctrl_deps: 252
data_deps: 299
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 298
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 300: 299 
Adding parent for node 300: 252 
Adding parent for node 300: 252 299 
Adding node 300
Message: id: 301
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 300
data_deps: 300
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 299
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 301: 300 
Adding parent for node 301: 300 
Adding parent for node 301: 300 
Adding node 301
Message: id: 302
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 301
data_deps: 301
inputs {
  values: "[[], [], 6, 0, \'cuda:0\', False]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 300
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 302: 301 
Adding parent for node 302: 301 
Adding parent for node 302: 301 
Adding node 302
Message: id: 303
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 300
data_deps: 302
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], 1.0]"
  shapes: "[[], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 301
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 303: 302 
Adding parent for node 303: 300 
Adding parent for node 303: 300 302 
Adding node 303
Message: id: 304
name: "autograd::engine::evaluate_function: MseLossBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 246
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 302
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 304: 246 
Adding parent for node 304: 150 
Adding parent for node 304: 150 246 
Adding node 304
Message: id: 305
name: "MseLossBackward0"
type: COMP_NODE
ctrl_deps: 304
data_deps: 304
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 303
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 28
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 305: 304 
Adding parent for node 305: 304 
Adding parent for node 305: 304 
Adding node 305
Message: id: 306
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 305
data_deps: 305
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], [356, 186, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[], [32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 304
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 306: 305 
Adding parent for node 306: 305 
Adding parent for node 306: 305 
Adding node 306
Message: id: 307
name: "aten::zeros_like"
type: COMP_NODE
ctrl_deps: 306
data_deps: 306
inputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'None\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 305
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 307: 306 
Adding parent for node 307: 306 
Adding parent for node 307: 306 
Adding node 307
Message: id: 308
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 307
data_deps: 307
inputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\'], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 306
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 308: 307 
Adding parent for node 308: 307 
Adding parent for node 308: 307 
Adding node 308
Message: id: 309
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 308
data_deps: 308
inputs {
  values: "[[32, 4], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 307
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 309: 308 
Adding parent for node 309: 308 
Adding parent for node 309: 308 
Adding node 309
Message: id: 310
name: "aten::zero_"
type: COMP_NODE
ctrl_deps: 307
data_deps: 309
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 308
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zero_(Tensor(a!) self) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 310: 309 
Adding parent for node 310: 307 
Adding parent for node 310: 307 309 
Adding node 310
Message: id: 311
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 310
data_deps: 310
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 4], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 309
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 311: 310 
Adding parent for node 311: 310 
Adding parent for node 311: 310 
Adding node 311
Message: id: 312
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 306
data_deps: 311
inputs {
  values: "[[176, 81, 0, 1, 4, \'cuda:0\'], [356, 186, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1, [41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[], [32, 4], [32, 4], [], [32, 4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Tensor(float)\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 310
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 312: 311 
Adding parent for node 312: 306 
Adding parent for node 312: 306 311 
Adding node 312
Message: id: 313
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 312
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 311
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 313: 312 
Adding parent for node 313: 150 
Adding parent for node 313: 150 312 
Adding node 313
Message: id: 314
name: "FullyShardedDataParallel._pre_backward_hook"
type: COMP_NODE
ctrl_deps: 313
data_deps: 313
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 312
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 314: 313 
Adding parent for node 314: 313 
Adding parent for node 314: 313 
Adding node 314
Message: id: 315
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 314
data_deps: 314
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 313
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 315: 314 
Adding parent for node 315: 314 
Adding parent for node 315: 314 
Adding node 315
Message: id: 316
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 315
data_deps: 315
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 314
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 316: 315 
Adding parent for node 316: 315 
Adding parent for node 316: 315 
Adding node 316
Message: id: 317
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 314
data_deps: 316
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 315
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 317: 316 
Adding parent for node 317: 314 
Adding parent for node 317: 314 316 
Adding node 317
Message: id: 318
name: "FullyShardedDataParallel._pre_backward_prefetch"
type: COMP_NODE
ctrl_deps: 314
data_deps: 317
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 316
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 318: 317 
Adding parent for node 318: 314 
Adding parent for node 318: 314 317 
Adding node 318
Message: id: 319
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 313
data_deps: 318
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 317
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 27
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 319: 318 
Adding parent for node 319: 313 
Adding parent for node 319: 313 318 
Adding node 319
Message: id: 320
name: "aten::t"
type: COMP_NODE
ctrl_deps: 319
data_deps: 319
inputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 318
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 320: 319 
Adding parent for node 320: 319 
Adding parent for node 320: 319 
Adding node 320
Message: id: 321
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 320
data_deps: 320
inputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 319
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 321: 320 
Adding parent for node 321: 320 
Adding parent for node 321: 320 
Adding node 321
Message: id: 322
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 321
data_deps: 321
inputs {
  values: "[[332, 135, 144, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[222, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 320
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 322: 321 
Adding parent for node 322: 321 
Adding parent for node 322: 321 
Adding node 322
Message: id: 323
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 319
data_deps: 322
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [222, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[209, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 321
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 323: 322 
Adding parent for node 323: 319 
Adding parent for node 323: 319 322 
Adding node 323
Message: id: 324
name: "aten::t"
type: COMP_NODE
ctrl_deps: 319
data_deps: 323
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 322
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 324: 323 
Adding parent for node 324: 319 
Adding parent for node 324: 319 323 
Adding node 324
Message: id: 325
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 324
data_deps: 324
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 323
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 325: 324 
Adding parent for node 325: 324 
Adding parent for node 325: 324 
Adding node 325
Message: id: 326
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 325
data_deps: 325
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [4, 32], [1, 4], \'<None>\']"
  shapes: "[[32, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 324
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 326: 325 
Adding parent for node 326: 325 
Adding parent for node 326: 325 
Adding node 326
Message: id: 327
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 319
data_deps: 326
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [246, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[4, 32], [32, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 325
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 327: 326 
Adding parent for node 327: 319 
Adding parent for node 327: 319 326 
Adding node 327
Message: id: 328
name: "aten::t"
type: COMP_NODE
ctrl_deps: 319
data_deps: 327
inputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 326
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 328: 327 
Adding parent for node 328: 319 
Adding parent for node 328: 319 327 
Adding node 328
Message: id: 329
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 328
data_deps: 328
inputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 327
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 329: 328 
Adding parent for node 329: 328 
Adding parent for node 329: 328 
Adding node 329
Message: id: 330
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 329
data_deps: 329
inputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 328
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 330: 329 
Adding parent for node 330: 329 
Adding parent for node 330: 329 
Adding node 330
Message: id: 331
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 313
data_deps: 330
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 4], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[222, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[1, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 329
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 331: 330 
Adding parent for node 331: 313 
Adding parent for node 331: 313 330 
Adding node 331
Message: id: 332
name: "aten::view"
type: COMP_NODE
ctrl_deps: 313
data_deps: 331
inputs {
  values: "[[222, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[1, 4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 330
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 332: 331 
Adding parent for node 332: 313 
Adding parent for node 332: 313 331 
Adding node 332
Message: id: 333
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 332
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 331
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 333: 332 
Adding parent for node 333: 150 
Adding parent for node 333: 150 332 
Adding node 333
Message: id: 334
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 333
data_deps: 333
inputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 332
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 26
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 334: 333 
Adding parent for node 334: 333 
Adding parent for node 334: 333 
Adding node 334
Message: id: 335
name: "aten::t"
type: COMP_NODE
ctrl_deps: 334
data_deps: 334
inputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 333
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 335: 334 
Adding parent for node 335: 334 
Adding parent for node 335: 334 
Adding node 335
Message: id: 336
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 335
data_deps: 335
inputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 334
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 336: 335 
Adding parent for node 336: 335 
Adding parent for node 336: 335 
Adding node 336
Message: id: 337
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 336
data_deps: 336
inputs {
  values: "[[397, 223, 0, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 335
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 337: 336 
Adding parent for node 337: 336 
Adding parent for node 337: 336 
Adding node 337
Message: id: 338
name: "autograd::engine::evaluate_function: ReluBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 337
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 336
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 338: 337 
Adding parent for node 338: 150 
Adding parent for node 338: 150 337 
Adding node 338
Message: id: 339
name: "ReluBackward0"
type: COMP_NODE
ctrl_deps: 338
data_deps: 338
inputs {
  values: "[[209, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 337
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 25
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 339: 338 
Adding parent for node 339: 338 
Adding parent for node 339: 338 
Adding node 339
Message: id: 340
name: "aten::threshold_backward"
type: COMP_NODE
ctrl_deps: 339
data_deps: 339
inputs {
  values: "[[209, 177, 0, 512, 4, \'cuda:0\'], [397, 166, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], [32, 16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 338
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 340: 339 
Adding parent for node 340: 339 
Adding parent for node 340: 339 
Adding node 340
Message: id: 341
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 340
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 339
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 341: 340 
Adding parent for node 341: 150 
Adding parent for node 341: 150 340 
Adding node 341
Message: id: 342
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 341
data_deps: 341
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 340
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 24
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 342: 341 
Adding parent for node 342: 341 
Adding parent for node 342: 341 
Adding node 342
Message: id: 343
name: "aten::t"
type: COMP_NODE
ctrl_deps: 342
data_deps: 342
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[326, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 341
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 343: 342 
Adding parent for node 343: 342 
Adding parent for node 343: 342 
Adding node 343
Message: id: 344
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 343
data_deps: 343
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[326, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 342
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 344: 343 
Adding parent for node 344: 343 
Adding parent for node 344: 343 
Adding node 344
Message: id: 345
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 344
data_deps: 344
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\'], [16, 32], [1, 16], \'<None>\']"
  shapes: "[[32, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[326, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 343
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 345: 344 
Adding parent for node 345: 344 
Adding parent for node 345: 344 
Adding node 345
Message: id: 346
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 342
data_deps: 345
inputs {
  values: "[[326, 240, 0, 512, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[16, 32], [32, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 344
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 346: 345 
Adding parent for node 346: 342 
Adding parent for node 346: 342 345 
Adding node 346
Message: id: 347
name: "aten::t"
type: COMP_NODE
ctrl_deps: 342
data_deps: 346
inputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 345
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 347: 346 
Adding parent for node 347: 342 
Adding parent for node 347: 342 346 
Adding node 347
Message: id: 348
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 347
data_deps: 347
inputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 346
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 348: 347 
Adding parent for node 348: 347 
Adding parent for node 348: 347 
Adding node 348
Message: id: 349
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 348
data_deps: 348
inputs {
  values: "[[209, 167, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 347
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 349: 348 
Adding parent for node 349: 348 
Adding parent for node 349: 348 
Adding node 349
Message: id: 350
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 341
data_deps: 349
inputs {
  values: "[[246, 240, 0, 512, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 16], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[326, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[1, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 348
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 350: 349 
Adding parent for node 350: 341 
Adding parent for node 350: 341 349 
Adding node 350
Message: id: 351
name: "aten::view"
type: COMP_NODE
ctrl_deps: 341
data_deps: 350
inputs {
  values: "[[326, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[1, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 349
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 351: 350 
Adding parent for node 351: 341 
Adding parent for node 351: 341 350 
Adding node 351
Message: id: 352
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 351
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 350
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 352: 351 
Adding parent for node 352: 150 
Adding parent for node 352: 150 351 
Adding node 352
Message: id: 353
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 352
data_deps: 352
inputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 351
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 23
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 353: 352 
Adding parent for node 353: 352 
Adding parent for node 353: 352 
Adding node 353
Message: id: 354
name: "aten::t"
type: COMP_NODE
ctrl_deps: 353
data_deps: 353
inputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 352
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 354: 353 
Adding parent for node 354: 353 
Adding parent for node 354: 353 
Adding node 354
Message: id: 355
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 354
data_deps: 354
inputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 353
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 355: 354 
Adding parent for node 355: 354 
Adding parent for node 355: 354 
Adding node 355
Message: id: 356
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 355
data_deps: 355
inputs {
  values: "[[397, 167, 0, 128, 4, \'cuda:0\'], [16, 8], [8, 1], \'<None>\']"
  shapes: "[[8, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 354
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 356: 355 
Adding parent for node 356: 355 
Adding parent for node 356: 355 
Adding node 356
Message: id: 357
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 356
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 355
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 357: 356 
Adding parent for node 357: 150 
Adding parent for node 357: 150 356 
Adding node 357
Message: id: 358
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 357
data_deps: 357
inputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 356
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 21
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 358: 357 
Adding parent for node 358: 357 
Adding parent for node 358: 357 
Adding node 358
Message: id: 359
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 358
data_deps: 358
inputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[397, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 357
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 359: 358 
Adding parent for node 359: 358 
Adding parent for node 359: 358 
Adding node 359
Message: id: 360
name: "aten::view"
type: COMP_NODE
ctrl_deps: 359
data_deps: 359
inputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[397, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 358
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 360: 359 
Adding parent for node 360: 359 
Adding parent for node 360: 359 
Adding node 360
Message: id: 361
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 360
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 359
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 361: 360 
Adding parent for node 361: 150 
Adding parent for node 361: 150 360 
Adding node 361
Message: id: 362
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 361
data_deps: 361
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 360
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 20
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 362: 361 
Adding parent for node 362: 361 
Adding parent for node 362: 361 
Adding node 362
Message: id: 363
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 362
data_deps: 362
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[400, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 361
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 363: 362 
Adding parent for node 363: 362 
Adding parent for node 363: 362 
Adding node 363
Message: id: 364
name: "aten::view"
type: COMP_NODE
ctrl_deps: 363
data_deps: 363
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[400, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 362
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 364: 363 
Adding parent for node 364: 363 
Adding parent for node 364: 363 
Adding node 364
Message: id: 365
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 364
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 363
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 365: 364 
Adding parent for node 365: 150 
Adding parent for node 365: 150 364 
Adding node 365
Message: id: 366
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 365
data_deps: 365
inputs {
  values: "[[41, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 364
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 19
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 366: 365 
Adding parent for node 366: 365 
Adding parent for node 366: 365 
Adding node 366
Message: id: 367
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 366
data_deps: 366
inputs {
  values: "[[41, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[332, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 365
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 367: 366 
Adding parent for node 367: 366 
Adding parent for node 367: 366 
Adding node 367
Message: id: 368
name: "aten::view"
type: COMP_NODE
ctrl_deps: 367
data_deps: 367
inputs {
  values: "[[41, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[332, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 366
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 368: 367 
Adding parent for node 368: 367 
Adding parent for node 368: 367 
Adding node 368
Message: id: 369
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 368
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 367
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 369: 368 
Adding parent for node 369: 150 
Adding parent for node 369: 150 368 
Adding node 369
Message: id: 370
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 369
data_deps: 369
inputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 368
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 18
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 370: 369 
Adding parent for node 370: 369 
Adding parent for node 370: 369 
Adding node 370
Message: id: 371
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 370
data_deps: 370
inputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 369
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 371: 370 
Adding parent for node 371: 370 
Adding parent for node 371: 370 
Adding node 371
Message: id: 372
name: "aten::view"
type: COMP_NODE
ctrl_deps: 371
data_deps: 371
inputs {
  values: "[[246, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 370
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 372: 371 
Adding parent for node 372: 371 
Adding parent for node 372: 371 
Adding node 372
Message: id: 373
name: "autograd::engine::evaluate_function: SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 372
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 371
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 373: 372 
Adding parent for node 373: 150 
Adding parent for node 373: 150 372 
Adding node 373
Message: id: 374
name: "SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 373
data_deps: 373
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [332, 252, 0, 16, 4, \'cuda:0\'], [400, 223, 0, 64, 4, \'cuda:0\'], [397, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[128], [16], [64], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 372
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: 17
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 374: 373 
Adding parent for node 374: 373 
Adding parent for node 374: 373 
Adding node 374
Message: id: 375
name: "aten::cat"
type: COMP_NODE
ctrl_deps: 374
data_deps: 374
inputs {
  values: "[[[41, 167, 0, 128, 4, \'cuda:0\'], [332, 252, 0, 16, 4, \'cuda:0\'], [400, 223, 0, 64, 4, \'cuda:0\'], [397, 229, 0, 4, 4, \'cuda:0\']], 0]"
  shapes: "[[[128], [16], [64], [4]], []]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\', \'Int\']"
}
outputs {
  values: "[[246, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 373
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::cat(Tensor[] tensors, int dim=0) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 375: 374 
Adding parent for node 375: 374 
Adding parent for node 375: 374 
Adding node 375
Message: id: 376
name: "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 150
data_deps: 375
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 374
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 376: 375 
Adding parent for node 376: 150 
Adding parent for node 376: 150 375 
Adding node 376
Message: id: 377
name: "torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 376
data_deps: 376
inputs {
  values: "[[246, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 375
}
attr {
  name: "fw_parent"
  int64_val: 311
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 377: 376 
Adding parent for node 377: 376 
Adding parent for node 377: 376 
Adding node 377
Message: id: 378
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 377
data_deps: 377
inputs {
  values: "[[246, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[400, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 376
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 378: 377 
Adding parent for node 378: 377 
Adding parent for node 378: 377 
Adding node 378
Message: id: 379
name: "detach"
type: COMP_NODE
ctrl_deps: 378
data_deps: 378
inputs {
  values: "[[246, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 377
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 379: 378 
Adding parent for node 379: 378 
Adding parent for node 379: 378 
Adding node 379
Message: id: 380
name: "FullyShardedDataParallel._post_backward_hook"
type: COMP_NODE
ctrl_deps: 376
data_deps: 379
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 378
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 380: 379 
Adding parent for node 380: 376 
Adding parent for node 380: 376 379 
Adding node 380
Message: id: 381
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 380
data_deps: 380
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 379
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 381: 380 
Adding parent for node 381: 380 
Adding parent for node 381: 380 
Adding node 381
Message: id: 382
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 380
data_deps: 381
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 380
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 382: 381 
Adding parent for node 382: 380 
Adding parent for node 382: 380 381 
Adding node 382
Message: id: 383
name: "FullyShardedDataParallel._post_backward_prefetch"
type: COMP_NODE
ctrl_deps: 380
data_deps: 382
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 381
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 383: 382 
Adding parent for node 383: 380 
Adding parent for node 383: 380 382 
Adding node 383
Message: id: 384
name: "aten::chunk"
type: COMP_NODE
ctrl_deps: 380
data_deps: 383
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 2, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[400, 279, 0, 106, 4, \'cuda:0\'], [332, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 382
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 384: 383 
Adding parent for node 384: 380 
Adding parent for node 384: 380 383 
Adding node 384
Message: id: 385
name: "aten::split"
type: COMP_NODE
ctrl_deps: 384
data_deps: 384
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 106, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[400, 279, 0, 106, 4, \'cuda:0\'], [332, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 383
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 385: 384 
Adding parent for node 385: 384 
Adding parent for node 385: 384 
Adding node 385
Message: id: 386
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 385
data_deps: 385
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[400, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 384
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 386: 385 
Adding parent for node 386: 385 
Adding parent for node 386: 385 
Adding node 386
Message: id: 387
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 386
data_deps: 386
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[400, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 385
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 387: 386 
Adding parent for node 387: 386 
Adding parent for node 387: 386 
Adding node 387
Message: id: 388
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 387
data_deps: 387
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[400, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 386
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 388: 387 
Adding parent for node 388: 387 
Adding parent for node 388: 387 
Adding node 388
Message: id: 389
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 385
data_deps: 388
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 387
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 389: 388 
Adding parent for node 389: 385 
Adding parent for node 389: 385 388 
Adding node 389
Message: id: 390
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 389
data_deps: 389
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 388
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 390: 389 
Adding parent for node 390: 389 
Adding parent for node 390: 389 
Adding node 390
Message: id: 391
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 390
data_deps: 390
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 106]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[332, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 389
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 391: 390 
Adding parent for node 391: 390 
Adding parent for node 391: 390 
Adding node 391
Message: id: 392
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 380
data_deps: 391
inputs {
  values: "[[400, 279, 0, 106, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, \'<None>\']"
  shapes: "[[106], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 390
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 392: 391 
Adding parent for node 392: 380 
Adding parent for node 392: 380 391 
Adding node 392
Message: id: 393
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 392
data_deps: 392
inputs {
  values: "[[106], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 391
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 393: 392 
Adding parent for node 393: 392 
Adding parent for node 393: 392 
Adding node 393
Message: id: 394
name: "aten::div_"
type: COMP_NODE
ctrl_deps: 380
data_deps: 393
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], [400, 464, 0, 1, 8, \'cpu\']]"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Tensor(double)\']"
}
outputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 392
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 394: 393 
Adding parent for node 394: 380 
Adding parent for node 394: 380 393 
Adding node 394
Message: id: 395
name: "c10d::_reduce_scatter_base_"
type: COMP_NODE
ctrl_deps: 380
data_deps: 394
inputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\'], [231, 279, 0, 212, 4, \'cuda:0\'], \'<Object>\', \'<Object>\', False, -1]"
  shapes: "[[106], [212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 393
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_reduce_scatter_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, __torch__.torch.classes.c10d.ReduceOp reduce_op, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 395: 394 
Adding parent for node 395: 380 
Adding parent for node 395: 380 394 
Adding node 395
Message: id: 396
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 395
data_deps: 395
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], [4, False], [\'0\', \'default_pg\'], 0, \'_reduce_scatter_base\', [], [], 0, 1, 2]"
  shapes: "[[212], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 394
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 396: 395 
Adding parent for node 396: 395 
Adding parent for node 396: 395 
Adding node 396
Message: id: 397
name: "nccl:_reduce_scatter_base"
type: COMP_NODE
ctrl_deps: 396
data_deps: 396
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 395
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 397: 396 
Adding parent for node 397: 396 
Adding parent for node 397: 396 
Adding node 397
Message: id: 398
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 380
data_deps: 397
inputs {
  values: "[[4, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 396
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 398: 397 
Adding parent for node 398: 380 
Adding parent for node 398: 380 397 
Adding node 398
Message: id: 399
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 380
data_deps: 398
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 397
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 399: 398 
Adding parent for node 399: 380 
Adding parent for node 399: 380 398 
Adding node 399
Message: id: 400
name: "Optimizer.step#SGD.step"
type: COMP_NODE
ctrl_deps: 252
data_deps: 303
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 398
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 400: 303 
Adding parent for node 400: 252 
Adding parent for node 400: 252 303 
Adding node 400
Message: id: 401
name: "aten::_foreach_add_"
type: COMP_NODE
ctrl_deps: 400
data_deps: 400
inputs {
  values: "[[[62, 107, 0, 106, 4, \'cuda:0\']], [[462, 298, 0, 106, 4, \'cuda:0\']], -0.01]"
  shapes: "[[[106]], [[106]], []]"
  types: "[\'GenericList[Tensor(float)]\', \'GenericList[Tensor(float)]\', \'Double\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 399
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 401: 400 
Adding parent for node 401: 400 
Adding parent for node 401: 400 
Adding node 401
Message: id: 402
name: "aten::result_type"
type: COMP_NODE
ctrl_deps: 401
data_deps: 401
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], -0.01]"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[6]"
  shapes: "[[]]"
  types: "[\'Int\']"
}
attr {
  name: "rf_id"
  int64_val: 400
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 402: 401 
Adding parent for node 402: 401 
Adding parent for node 402: 401 
Adding node 402
Message: id: 403
name: "aten::item"
type: COMP_NODE
ctrl_deps: 252
data_deps: 402
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.072134]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 401
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::item(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 403: 402 
Adding parent for node 403: 252 
Adding parent for node 403: 252 402 
Adding node 403
Message: id: 404
name: "aten::_local_scalar_dense"
type: COMP_NODE
ctrl_deps: 403
data_deps: 403
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.072134]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 402
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_local_scalar_dense(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 404: 403 
Adding parent for node 404: 403 
Adding parent for node 404: 403 
Adding node 404
Message: id: 405
name: "ProfilerStep#2"
type: COMP_NODE
ctrl_deps: 1
data_deps: 404
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 403
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 405: 404 
Adding parent for node 405: 1 
Adding parent for node 405: 1 404 
Adding node 405
Message: id: 406
name: "Optimizer.zero_grad#SGD.zero_grad"
type: COMP_NODE
ctrl_deps: 405
data_deps: 405
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 404
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 406: 405 
Adding parent for node 406: 405 
Adding parent for node 406: 405 
Adding node 406
Message: id: 407
name: "FullyShardedDataParallel.forward"
type: COMP_NODE
ctrl_deps: 405
data_deps: 406
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 405
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 407: 406 
Adding parent for node 407: 405 
Adding parent for node 407: 405 406 
Adding node 407
Message: id: 408
name: "FullyShardedDataParallel._root_pre_forward"
type: COMP_NODE
ctrl_deps: 407
data_deps: 407
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 406
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 408: 407 
Adding parent for node 408: 407 
Adding parent for node 408: 407 
Adding node 408
Message: id: 409
name: "FullyShardedDataParallel._to_kwargs"
type: COMP_NODE
ctrl_deps: 408
data_deps: 408
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 407
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 409: 408 
Adding parent for node 409: 408 
Adding parent for node 409: 408 
Adding node 409
Message: id: 410
name: "FullyShardedDataParallel._pre_forward"
type: COMP_NODE
ctrl_deps: 407
data_deps: 409
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 408
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 410: 409 
Adding parent for node 410: 407 
Adding parent for node 410: 407 409 
Adding node 410
Message: id: 411
name: "FullyShardedDataParallel.rate_limiter"
type: COMP_NODE
ctrl_deps: 410
data_deps: 410
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 409
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 411: 410 
Adding parent for node 411: 410 
Adding parent for node 411: 410 
Adding node 411
Message: id: 412
name: "c10d::_allgather_base_"
type: COMP_NODE
ctrl_deps: 410
data_deps: 411
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [462, 107, 0, 106, 4, \'cuda:0\'], \'<Object>\', False, -1]"
  shapes: "[[212], [106], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 410
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_allgather_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 412: 411 
Adding parent for node 412: 410 
Adding parent for node 412: 410 411 
Adding node 412
Message: id: 413
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 412
data_deps: 412
inputs {
  values: "[[462, 107, 0, 106, 4, \'cuda:0\'], [5, False], [\'0\', \'default_pg\'], 0, \'_allgather_base\', [], [], 0, 1, 2]"
  shapes: "[[106], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 411
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 413: 412 
Adding parent for node 413: 412 
Adding parent for node 413: 412 
Adding node 413
Message: id: 414
name: "nccl:_all_gather_base"
type: COMP_NODE
ctrl_deps: 413
data_deps: 413
inputs {
  values: "[[462, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 412
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 414: 413 
Adding parent for node 414: 413 
Adding parent for node 414: 413 
Adding node 414
Message: id: 415
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 410
data_deps: 414
inputs {
  values: "[[5, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 413
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 415: 414 
Adding parent for node 415: 410 
Adding parent for node 415: 410 414 
Adding node 415
Message: id: 416
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 410
data_deps: 415
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 414
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 416: 415 
Adding parent for node 416: 410 
Adding parent for node 416: 410 415 
Adding node 416
Message: id: 417
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 416
data_deps: 416
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 415
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 417: 416 
Adding parent for node 417: 416 
Adding parent for node 417: 416 
Adding node 417
Message: id: 418
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 410
data_deps: 417
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 416
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 418: 417 
Adding parent for node 418: 410 
Adding parent for node 418: 410 417 
Adding node 418
Message: id: 419
name: "aten::split_with_sizes"
type: COMP_NODE
ctrl_deps: 410
data_deps: 418
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128, 16, 64, 4], 0]"
  shapes: "[[212], [[], [], [], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int,Int,Int]\', \'Int\']"
}
outputs {
  values: "[[[176, 135, 0, 128, 4, \'cuda:0\'], [93, 135, 128, 16, 4, \'cuda:0\'], [365, 135, 144, 64, 4, \'cuda:0\'], [494, 135, 208, 4, 4, \'cuda:0\']]]"
  shapes: "[[[128], [16], [64], [4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 417
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 419: 418 
Adding parent for node 419: 410 
Adding parent for node 419: 410 418 
Adding node 419
Message: id: 420
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 419
data_deps: 419
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[176, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 418
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 420: 419 
Adding parent for node 420: 419 
Adding parent for node 420: 419 
Adding node 420
Message: id: 421
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 419
data_deps: 420
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [16], [1], 128]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[93, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 419
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 421: 420 
Adding parent for node 421: 419 
Adding parent for node 421: 419 420 
Adding node 421
Message: id: 422
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 419
data_deps: 421
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [64], [1], 144]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[365, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 420
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 422: 421 
Adding parent for node 422: 419 
Adding parent for node 422: 419 421 
Adding node 422
Message: id: 423
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 419
data_deps: 422
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [4], [1], 208]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[494, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 421
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 423: 422 
Adding parent for node 423: 419 
Adding parent for node 423: 419 422 
Adding node 423
Message: id: 424
name: "aten::view"
type: COMP_NODE
ctrl_deps: 410
data_deps: 423
inputs {
  values: "[[176, 135, 0, 128, 4, \'cuda:0\'], [16, 8]]"
  shapes: "[[128], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[496, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 422
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 30
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 424: 423 
Adding parent for node 424: 410 
Adding parent for node 424: 410 423 
Adding node 424
Message: id: 425
name: "aten::view"
type: COMP_NODE
ctrl_deps: 410
data_deps: 424
inputs {
  values: "[[93, 135, 128, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[334, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 423
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 31
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 425: 424 
Adding parent for node 425: 410 
Adding parent for node 425: 410 424 
Adding node 425
Message: id: 426
name: "aten::view"
type: COMP_NODE
ctrl_deps: 410
data_deps: 425
inputs {
  values: "[[365, 135, 144, 64, 4, \'cuda:0\'], [4, 16]]"
  shapes: "[[64], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[152, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 424
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 32
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 426: 425 
Adding parent for node 426: 410 
Adding parent for node 426: 410 425 
Adding node 426
Message: id: 427
name: "aten::view"
type: COMP_NODE
ctrl_deps: 410
data_deps: 426
inputs {
  values: "[[494, 135, 208, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[77, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 425
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 33
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 427: 426 
Adding parent for node 427: 410 
Adding parent for node 427: 410 426 
Adding node 427
Message: id: 428
name: "FullyShardedDataParallel._pre_forward_prefetch"
type: COMP_NODE
ctrl_deps: 410
data_deps: 427
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 426
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 428: 427 
Adding parent for node 428: 410 
Adding parent for node 428: 410 427 
Adding node 428
Message: id: 429
name: "aten::expand_as"
type: COMP_NODE
ctrl_deps: 410
data_deps: 428
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [62, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 427
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 34
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 429: 428 
Adding parent for node 429: 410 
Adding parent for node 429: 410 428 
Adding node 429
Message: id: 430
name: "aten::expand"
type: COMP_NODE
ctrl_deps: 429
data_deps: 429
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], False]"
  shapes: "[[212], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\']"
}
outputs {
  values: "[[462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 428
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 34
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 430: 429 
Adding parent for node 430: 429 
Adding parent for node 430: 429 
Adding node 430
Message: id: 431
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 430
data_deps: 430
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], [1], \'<None>\']"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'None\']"
}
outputs {
  values: "[[462, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 429
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 431: 430 
Adding parent for node 431: 430 
Adding parent for node 431: 430 
Adding node 431
Message: id: 432
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 407
data_deps: 431
inputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\'], [496, 135, 0, 128, 4, \'cuda:0\'], [334, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[32, 8], [16, 8], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[494, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 430
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 35
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 432: 431 
Adding parent for node 432: 407 
Adding parent for node 432: 407 431 
Adding node 432
Message: id: 433
name: "aten::t"
type: COMP_NODE
ctrl_deps: 432
data_deps: 432
inputs {
  values: "[[496, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[462, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 431
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 35
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 433: 432 
Adding parent for node 433: 432 
Adding parent for node 433: 432 
Adding node 433
Message: id: 434
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 433
data_deps: 433
inputs {
  values: "[[496, 135, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[462, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 432
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 434: 433 
Adding parent for node 434: 433 
Adding parent for node 434: 433 
Adding node 434
Message: id: 435
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 434
data_deps: 434
inputs {
  values: "[[496, 135, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[462, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 433
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 435: 434 
Adding parent for node 435: 434 
Adding parent for node 435: 434 
Adding node 435
Message: id: 436
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 432
data_deps: 435
inputs {
  values: "[[334, 135, 128, 16, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\'], [462, 135, 0, 128, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[16], [32, 8], [8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 434
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 36
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 436: 435 
Adding parent for node 436: 432 
Adding parent for node 436: 432 435 
Adding node 436
Message: id: 437
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 436
data_deps: 436
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[176, 166, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 435
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 437: 436 
Adding parent for node 437: 436 
Adding parent for node 437: 436 
Adding node 437
Message: id: 438
name: "aten::relu"
type: COMP_NODE
ctrl_deps: 407
data_deps: 437
inputs {
  values: "[[494, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[462, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 436
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 37
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::relu(Tensor self) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 438: 437 
Adding parent for node 438: 407 
Adding parent for node 438: 407 437 
Adding node 438
Message: id: 439
name: "aten::clamp_min"
type: COMP_NODE
ctrl_deps: 438
data_deps: 438
inputs {
  values: "[[494, 167, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[462, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 437
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::clamp_min(Tensor self, Scalar min) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 439: 438 
Adding parent for node 439: 438 
Adding parent for node 439: 438 
Adding node 439
Message: id: 440
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 407
data_deps: 439
inputs {
  values: "[[462, 166, 0, 512, 4, \'cuda:0\'], [152, 135, 144, 64, 4, \'cuda:0\'], [77, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[32, 16], [4, 16], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 438
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 38
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 440: 439 
Adding parent for node 440: 407 
Adding parent for node 440: 407 439 
Adding node 440
Message: id: 441
name: "aten::t"
type: COMP_NODE
ctrl_deps: 440
data_deps: 440
inputs {
  values: "[[152, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 439
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 38
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 441: 440 
Adding parent for node 441: 440 
Adding parent for node 441: 440 
Adding node 441
Message: id: 442
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 441
data_deps: 441
inputs {
  values: "[[152, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 440
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 442: 441 
Adding parent for node 442: 441 
Adding parent for node 442: 441 
Adding node 442
Message: id: 443
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 442
data_deps: 442
inputs {
  values: "[[152, 135, 144, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 441
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 443: 442 
Adding parent for node 443: 442 
Adding parent for node 443: 442 
Adding node 443
Message: id: 444
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 440
data_deps: 443
inputs {
  values: "[[77, 135, 208, 4, 4, \'cuda:0\'], [462, 166, 0, 512, 4, \'cuda:0\'], [494, 135, 144, 64, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[4], [32, 16], [16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 442
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 39
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 444: 443 
Adding parent for node 444: 440 
Adding parent for node 444: 440 443 
Adding node 444
Message: id: 445
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 444
data_deps: 444
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[93, 177, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 443
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 445: 444 
Adding parent for node 445: 444 
Adding parent for node 445: 444 
Adding node 445
Message: id: 446
name: "FullyShardedDataParallel._post_forward"
type: COMP_NODE
ctrl_deps: 407
data_deps: 445
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 444
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 446: 445 
Adding parent for node 446: 407 
Adding parent for node 446: 407 445 
Adding node 446
Message: id: 447
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 446
data_deps: 446
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 445
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 40
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 447: 446 
Adding parent for node 447: 446 
Adding parent for node 447: 446 
Adding node 447
Message: id: 448
name: "aten::broadcast_tensors"
type: COMP_NODE
ctrl_deps: 405
data_deps: 447
inputs {
  values: "[[[365, 81, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
outputs {
  values: "[[[365, 81, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 446
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 40
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 448: 447 
Adding parent for node 448: 405 
Adding parent for node 448: 405 447 
Adding node 448
Message: id: 449
name: "aten::mse_loss"
type: COMP_NODE
ctrl_deps: 405
data_deps: 448
inputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 447
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 40
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 449: 448 
Adding parent for node 449: 405 
Adding parent for node 449: 405 448 
Adding node 449
Message: id: 450
name: "aten::resize_"
type: COMP_NODE
ctrl_deps: 449
data_deps: 449
inputs {
  values: "[[356, 186, 0, 128, 4, \'cuda:0\'], [], \'<None>\']"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'None\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 448
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 450: 449 
Adding parent for node 450: 449 
Adding parent for node 450: 449 
Adding node 450
Message: id: 451
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 449
data_deps: 450
inputs {
  values: "[[32, 4], 6, \'<None>\', \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'None\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 449
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 451: 450 
Adding parent for node 451: 449 
Adding parent for node 451: 449 450 
Adding node 451
Message: id: 452
name: "aten::mean"
type: COMP_NODE
ctrl_deps: 449
data_deps: 451
inputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\'], [], False, \'<None>\', [356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [], [], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'Bool\', \'None\', \'Tensor(float)\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 450
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 452: 451 
Adding parent for node 452: 449 
Adding parent for node 452: 449 451 
Adding node 452
Message: id: 453
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 452
data_deps: 452
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], [1, 1], [0, 0], \'<None>\']"
  shapes: "[[], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[91, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[1, 1]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 451
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 453: 452 
Adding parent for node 453: 452 
Adding parent for node 453: 452 
Adding node 453
Message: id: 454
name: "aten::ones_like"
type: COMP_NODE
ctrl_deps: 405
data_deps: 453
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 452
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 454: 453 
Adding parent for node 454: 405 
Adding parent for node 454: 405 453 
Adding node 454
Message: id: 455
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 454
data_deps: 454
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 453
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 455: 454 
Adding parent for node 455: 454 
Adding parent for node 455: 454 
Adding node 455
Message: id: 456
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 455
data_deps: 455
inputs {
  values: "[[], [], 6, 0, \'cuda:0\', False]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 454
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 456: 455 
Adding parent for node 456: 455 
Adding parent for node 456: 455 
Adding node 456
Message: id: 457
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 454
data_deps: 456
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], 1.0]"
  shapes: "[[], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 455
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 457: 456 
Adding parent for node 457: 454 
Adding parent for node 457: 454 456 
Adding node 457
Message: id: 458
name: "autograd::engine::evaluate_function: MseLossBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 399
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 456
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 458: 399 
Adding parent for node 458: 150 
Adding parent for node 458: 150 399 
Adding node 458
Message: id: 459
name: "MseLossBackward0"
type: COMP_NODE
ctrl_deps: 458
data_deps: 458
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 457
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 40
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 459: 458 
Adding parent for node 459: 458 
Adding parent for node 459: 458 
Adding node 459
Message: id: 460
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 459
data_deps: 459
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], [365, 81, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[], [32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 458
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 460: 459 
Adding parent for node 460: 459 
Adding parent for node 460: 459 
Adding node 460
Message: id: 461
name: "aten::zeros_like"
type: COMP_NODE
ctrl_deps: 460
data_deps: 460
inputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'None\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 459
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 461: 460 
Adding parent for node 461: 460 
Adding parent for node 461: 460 
Adding node 461
Message: id: 462
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 461
data_deps: 461
inputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\'], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 460
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 462: 461 
Adding parent for node 462: 461 
Adding parent for node 462: 461 
Adding node 462
Message: id: 463
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 462
data_deps: 462
inputs {
  values: "[[32, 4], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 461
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 463: 462 
Adding parent for node 463: 462 
Adding parent for node 463: 462 
Adding node 463
Message: id: 464
name: "aten::zero_"
type: COMP_NODE
ctrl_deps: 461
data_deps: 463
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 462
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zero_(Tensor(a!) self) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 464: 463 
Adding parent for node 464: 461 
Adding parent for node 464: 461 463 
Adding node 464
Message: id: 465
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 464
data_deps: 464
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 4], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 463
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 465: 464 
Adding parent for node 465: 464 
Adding parent for node 465: 464 
Adding node 465
Message: id: 466
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 460
data_deps: 465
inputs {
  values: "[[178, 56, 0, 1, 4, \'cuda:0\'], [365, 81, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1, [222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[], [32, 4], [32, 4], [], [32, 4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Tensor(float)\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 464
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 466: 465 
Adding parent for node 466: 460 
Adding parent for node 466: 460 465 
Adding node 466
Message: id: 467
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 466
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 465
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 467: 466 
Adding parent for node 467: 150 
Adding parent for node 467: 150 466 
Adding node 467
Message: id: 468
name: "FullyShardedDataParallel._pre_backward_hook"
type: COMP_NODE
ctrl_deps: 467
data_deps: 467
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 466
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 468: 467 
Adding parent for node 468: 467 
Adding parent for node 468: 467 
Adding node 468
Message: id: 469
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 468
data_deps: 468
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[400, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 467
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 469: 468 
Adding parent for node 469: 468 
Adding parent for node 469: 468 
Adding node 469
Message: id: 470
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 469
data_deps: 469
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[400, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 468
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 470: 469 
Adding parent for node 470: 469 
Adding parent for node 470: 469 
Adding node 470
Message: id: 471
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 468
data_deps: 470
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [400, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 469
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 471: 470 
Adding parent for node 471: 468 
Adding parent for node 471: 468 470 
Adding node 471
Message: id: 472
name: "FullyShardedDataParallel._pre_backward_prefetch"
type: COMP_NODE
ctrl_deps: 468
data_deps: 471
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 470
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 472: 471 
Adding parent for node 472: 468 
Adding parent for node 472: 468 471 
Adding node 472
Message: id: 473
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 467
data_deps: 472
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 471
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 39
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 473: 472 
Adding parent for node 473: 467 
Adding parent for node 473: 467 472 
Adding node 473
Message: id: 474
name: "aten::t"
type: COMP_NODE
ctrl_deps: 473
data_deps: 473
inputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[400, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 472
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 474: 473 
Adding parent for node 474: 473 
Adding parent for node 474: 473 
Adding node 474
Message: id: 475
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 474
data_deps: 474
inputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[400, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 473
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 475: 474 
Adding parent for node 475: 474 
Adding parent for node 475: 474 
Adding node 475
Message: id: 476
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 475
data_deps: 475
inputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[400, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 474
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 476: 475 
Adding parent for node 476: 475 
Adding parent for node 476: 475 
Adding node 476
Message: id: 477
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 473
data_deps: 476
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [400, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[231, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 475
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 477: 476 
Adding parent for node 477: 473 
Adding parent for node 477: 473 476 
Adding node 477
Message: id: 478
name: "aten::t"
type: COMP_NODE
ctrl_deps: 473
data_deps: 477
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[400, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 476
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 478: 477 
Adding parent for node 478: 473 
Adding parent for node 478: 473 477 
Adding node 478
Message: id: 479
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 478
data_deps: 478
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[400, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 477
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 479: 478 
Adding parent for node 479: 478 
Adding parent for node 479: 478 
Adding node 479
Message: id: 480
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 479
data_deps: 479
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [4, 32], [1, 4], \'<None>\']"
  shapes: "[[32, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[400, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 478
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 480: 479 
Adding parent for node 480: 479 
Adding parent for node 480: 479 
Adding node 480
Message: id: 481
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 473
data_deps: 480
inputs {
  values: "[[400, 167, 0, 128, 4, \'cuda:0\'], [462, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[4, 32], [32, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 479
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 481: 480 
Adding parent for node 481: 473 
Adding parent for node 481: 473 480 
Adding node 481
Message: id: 482
name: "aten::t"
type: COMP_NODE
ctrl_deps: 473
data_deps: 481
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 480
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 482: 481 
Adding parent for node 482: 473 
Adding parent for node 482: 473 481 
Adding node 482
Message: id: 483
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 482
data_deps: 482
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 481
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 483: 482 
Adding parent for node 483: 482 
Adding parent for node 483: 482 
Adding node 483
Message: id: 484
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 483
data_deps: 483
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 482
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 484: 483 
Adding parent for node 484: 483 
Adding parent for node 484: 483 
Adding node 484
Message: id: 485
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 467
data_deps: 484
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 4], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[1, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 483
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 485: 484 
Adding parent for node 485: 467 
Adding parent for node 485: 467 484 
Adding node 485
Message: id: 486
name: "aten::view"
type: COMP_NODE
ctrl_deps: 467
data_deps: 485
inputs {
  values: "[[400, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[1, 4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 484
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 486: 485 
Adding parent for node 486: 467 
Adding parent for node 486: 467 485 
Adding node 486
Message: id: 487
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 486
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 485
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 487: 486 
Adding parent for node 487: 150 
Adding parent for node 487: 150 486 
Adding node 487
Message: id: 488
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 487
data_deps: 487
inputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 486
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 38
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 488: 487 
Adding parent for node 488: 487 
Adding parent for node 488: 487 
Adding node 488
Message: id: 489
name: "aten::t"
type: COMP_NODE
ctrl_deps: 488
data_deps: 488
inputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 487
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 489: 488 
Adding parent for node 489: 488 
Adding parent for node 489: 488 
Adding node 489
Message: id: 490
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 489
data_deps: 489
inputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 488
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 490: 489 
Adding parent for node 490: 489 
Adding parent for node 490: 489 
Adding node 490
Message: id: 491
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 490
data_deps: 490
inputs {
  values: "[[326, 223, 0, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 489
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 491: 490 
Adding parent for node 491: 490 
Adding parent for node 491: 490 
Adding node 491
Message: id: 492
name: "autograd::engine::evaluate_function: ReluBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 491
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 490
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 492: 491 
Adding parent for node 492: 150 
Adding parent for node 492: 150 491 
Adding node 492
Message: id: 493
name: "ReluBackward0"
type: COMP_NODE
ctrl_deps: 492
data_deps: 492
inputs {
  values: "[[231, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 491
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 37
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 493: 492 
Adding parent for node 493: 492 
Adding parent for node 493: 492 
Adding node 493
Message: id: 494
name: "aten::threshold_backward"
type: COMP_NODE
ctrl_deps: 493
data_deps: 493
inputs {
  values: "[[231, 177, 0, 512, 4, \'cuda:0\'], [326, 166, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], [32, 16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 492
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 494: 493 
Adding parent for node 494: 493 
Adding parent for node 494: 493 
Adding node 494
Message: id: 495
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 494
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 493
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 495: 494 
Adding parent for node 495: 150 
Adding parent for node 495: 150 494 
Adding node 495
Message: id: 496
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 495
data_deps: 495
inputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 494
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 36
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 496: 495 
Adding parent for node 496: 495 
Adding parent for node 496: 495 
Adding node 496
Message: id: 497
name: "aten::t"
type: COMP_NODE
ctrl_deps: 496
data_deps: 496
inputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[176, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 495
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 497: 496 
Adding parent for node 497: 496 
Adding parent for node 497: 496 
Adding node 497
Message: id: 498
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 497
data_deps: 497
inputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[176, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 496
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 498: 497 
Adding parent for node 498: 497 
Adding parent for node 498: 497 
Adding node 498
Message: id: 499
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 498
data_deps: 498
inputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\'], [16, 32], [1, 16], \'<None>\']"
  shapes: "[[32, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[176, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 497
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 499: 498 
Adding parent for node 499: 498 
Adding parent for node 499: 498 
Adding node 499
Message: id: 500
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 496
data_deps: 499
inputs {
  values: "[[176, 240, 0, 512, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[16, 32], [32, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[231, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 498
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 500: 499 
Adding parent for node 500: 496 
Adding parent for node 500: 496 499 
Adding node 500
Message: id: 501
name: "aten::t"
type: COMP_NODE
ctrl_deps: 496
data_deps: 500
inputs {
  values: "[[231, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 499
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 501: 500 
Adding parent for node 501: 496 
Adding parent for node 501: 496 500 
Adding node 501
Message: id: 502
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 501
data_deps: 501
inputs {
  values: "[[231, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 500
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 502: 501 
Adding parent for node 502: 501 
Adding parent for node 502: 501 
Adding node 502
Message: id: 503
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 502
data_deps: 502
inputs {
  values: "[[231, 167, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 501
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 503: 502 
Adding parent for node 503: 502 
Adding parent for node 503: 502 
Adding node 503
Message: id: 504
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 495
data_deps: 503
inputs {
  values: "[[462, 240, 0, 512, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 16], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[176, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[1, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 502
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 504: 503 
Adding parent for node 504: 495 
Adding parent for node 504: 495 503 
Adding node 504
Message: id: 505
name: "aten::view"
type: COMP_NODE
ctrl_deps: 495
data_deps: 504
inputs {
  values: "[[176, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[1, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[222, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 503
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 505: 504 
Adding parent for node 505: 495 
Adding parent for node 505: 495 504 
Adding node 505
Message: id: 506
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 505
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 504
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 506: 505 
Adding parent for node 506: 150 
Adding parent for node 506: 150 505 
Adding node 506
Message: id: 507
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 506
data_deps: 506
inputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 505
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 35
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 507: 506 
Adding parent for node 507: 506 
Adding parent for node 507: 506 
Adding node 507
Message: id: 508
name: "aten::t"
type: COMP_NODE
ctrl_deps: 507
data_deps: 507
inputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 506
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 508: 507 
Adding parent for node 508: 507 
Adding parent for node 508: 507 
Adding node 508
Message: id: 509
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 508
data_deps: 508
inputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 507
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 509: 508 
Adding parent for node 509: 508 
Adding parent for node 509: 508 
Adding node 509
Message: id: 510
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 509
data_deps: 509
inputs {
  values: "[[326, 167, 0, 128, 4, \'cuda:0\'], [16, 8], [8, 1], \'<None>\']"
  shapes: "[[8, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 508
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 510: 509 
Adding parent for node 510: 509 
Adding parent for node 510: 509 
Adding node 510
Message: id: 511
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 510
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 509
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 511: 510 
Adding parent for node 511: 150 
Adding parent for node 511: 150 510 
Adding node 511
Message: id: 512
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 511
data_deps: 511
inputs {
  values: "[[41, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 510
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 33
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 512: 511 
Adding parent for node 512: 511 
Adding parent for node 512: 511 
Adding node 512
Message: id: 513
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 512
data_deps: 512
inputs {
  values: "[[41, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[326, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 511
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 513: 512 
Adding parent for node 513: 512 
Adding parent for node 513: 512 
Adding node 513
Message: id: 514
name: "aten::view"
type: COMP_NODE
ctrl_deps: 513
data_deps: 513
inputs {
  values: "[[41, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[326, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 512
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 514: 513 
Adding parent for node 514: 513 
Adding parent for node 514: 513 
Adding node 514
Message: id: 515
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 514
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 513
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 515: 514 
Adding parent for node 515: 150 
Adding parent for node 515: 150 514 
Adding node 515
Message: id: 516
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 515
data_deps: 515
inputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 514
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 32
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 516: 515 
Adding parent for node 516: 515 
Adding parent for node 516: 515 
Adding node 516
Message: id: 517
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 516
data_deps: 516
inputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 515
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 517: 516 
Adding parent for node 517: 516 
Adding parent for node 517: 516 
Adding node 517
Message: id: 518
name: "aten::view"
type: COMP_NODE
ctrl_deps: 517
data_deps: 517
inputs {
  values: "[[494, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[41, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 516
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 518: 517 
Adding parent for node 518: 517 
Adding parent for node 518: 517 
Adding node 518
Message: id: 519
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 518
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 517
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 519: 518 
Adding parent for node 519: 150 
Adding parent for node 519: 150 518 
Adding node 519
Message: id: 520
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 519
data_deps: 519
inputs {
  values: "[[222, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 518
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 31
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 520: 519 
Adding parent for node 520: 519 
Adding parent for node 520: 519 
Adding node 520
Message: id: 521
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 520
data_deps: 520
inputs {
  values: "[[222, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[494, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 519
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 521: 520 
Adding parent for node 521: 520 
Adding parent for node 521: 520 
Adding node 521
Message: id: 522
name: "aten::view"
type: COMP_NODE
ctrl_deps: 521
data_deps: 521
inputs {
  values: "[[222, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[494, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 520
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 522: 521 
Adding parent for node 522: 521 
Adding parent for node 522: 521 
Adding node 522
Message: id: 523
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 522
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 521
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 523: 522 
Adding parent for node 523: 150 
Adding parent for node 523: 150 522 
Adding node 523
Message: id: 524
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 523
data_deps: 523
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 522
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 30
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 524: 523 
Adding parent for node 524: 523 
Adding parent for node 524: 523 
Adding node 524
Message: id: 525
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 524
data_deps: 524
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 523
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 525: 524 
Adding parent for node 525: 524 
Adding parent for node 525: 524 
Adding node 525
Message: id: 526
name: "aten::view"
type: COMP_NODE
ctrl_deps: 525
data_deps: 525
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 524
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 526: 525 
Adding parent for node 526: 525 
Adding parent for node 526: 525 
Adding node 526
Message: id: 527
name: "autograd::engine::evaluate_function: SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 526
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 525
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 527: 526 
Adding parent for node 527: 150 
Adding parent for node 527: 150 526 
Adding node 527
Message: id: 528
name: "SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 527
data_deps: 527
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [494, 252, 0, 16, 4, \'cuda:0\'], [41, 223, 0, 64, 4, \'cuda:0\'], [326, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[128], [16], [64], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 526
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: 29
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 528: 527 
Adding parent for node 528: 527 
Adding parent for node 528: 527 
Adding node 528
Message: id: 529
name: "aten::cat"
type: COMP_NODE
ctrl_deps: 528
data_deps: 528
inputs {
  values: "[[[222, 167, 0, 128, 4, \'cuda:0\'], [494, 252, 0, 16, 4, \'cuda:0\'], [41, 223, 0, 64, 4, \'cuda:0\'], [326, 229, 0, 4, 4, \'cuda:0\']], 0]"
  shapes: "[[[128], [16], [64], [4]], []]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\', \'Int\']"
}
outputs {
  values: "[[462, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 527
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::cat(Tensor[] tensors, int dim=0) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 529: 528 
Adding parent for node 529: 528 
Adding parent for node 529: 528 
Adding node 529
Message: id: 530
name: "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 150
data_deps: 529
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 528
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 530: 529 
Adding parent for node 530: 150 
Adding parent for node 530: 150 529 
Adding node 530
Message: id: 531
name: "torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 530
data_deps: 530
inputs {
  values: "[[462, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 529
}
attr {
  name: "fw_parent"
  int64_val: 475
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 531: 530 
Adding parent for node 531: 530 
Adding parent for node 531: 530 
Adding node 531
Message: id: 532
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 531
data_deps: 531
inputs {
  values: "[[462, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[332, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 530
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 532: 531 
Adding parent for node 532: 531 
Adding parent for node 532: 531 
Adding node 532
Message: id: 533
name: "detach"
type: COMP_NODE
ctrl_deps: 532
data_deps: 532
inputs {
  values: "[[462, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 531
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 533: 532 
Adding parent for node 533: 532 
Adding parent for node 533: 532 
Adding node 533
Message: id: 534
name: "FullyShardedDataParallel._post_backward_hook"
type: COMP_NODE
ctrl_deps: 530
data_deps: 533
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 532
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 534: 533 
Adding parent for node 534: 530 
Adding parent for node 534: 530 533 
Adding node 534
Message: id: 535
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 534
data_deps: 534
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 533
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 535: 534 
Adding parent for node 535: 534 
Adding parent for node 535: 534 
Adding node 535
Message: id: 536
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 534
data_deps: 535
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 534
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 536: 535 
Adding parent for node 536: 534 
Adding parent for node 536: 534 535 
Adding node 536
Message: id: 537
name: "FullyShardedDataParallel._post_backward_prefetch"
type: COMP_NODE
ctrl_deps: 534
data_deps: 536
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 535
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 537: 536 
Adding parent for node 537: 534 
Adding parent for node 537: 534 536 
Adding node 537
Message: id: 538
name: "aten::chunk"
type: COMP_NODE
ctrl_deps: 534
data_deps: 537
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 2, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[332, 279, 0, 106, 4, \'cuda:0\'], [176, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 536
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 538: 537 
Adding parent for node 538: 534 
Adding parent for node 538: 534 537 
Adding node 538
Message: id: 539
name: "aten::split"
type: COMP_NODE
ctrl_deps: 538
data_deps: 538
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 106, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[332, 279, 0, 106, 4, \'cuda:0\'], [176, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 537
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 539: 538 
Adding parent for node 539: 538 
Adding parent for node 539: 538 
Adding node 539
Message: id: 540
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 539
data_deps: 539
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 538
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 540: 539 
Adding parent for node 540: 539 
Adding parent for node 540: 539 
Adding node 540
Message: id: 541
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 540
data_deps: 540
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 539
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 541: 540 
Adding parent for node 541: 540 
Adding parent for node 541: 540 
Adding node 541
Message: id: 542
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 541
data_deps: 541
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 540
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 542: 541 
Adding parent for node 542: 541 
Adding parent for node 542: 541 
Adding node 542
Message: id: 543
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 539
data_deps: 542
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[176, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 541
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 543: 542 
Adding parent for node 543: 539 
Adding parent for node 543: 539 542 
Adding node 543
Message: id: 544
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 543
data_deps: 543
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[176, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 542
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 544: 543 
Adding parent for node 544: 543 
Adding parent for node 544: 543 
Adding node 544
Message: id: 545
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 544
data_deps: 544
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 106]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[176, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 543
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 545: 544 
Adding parent for node 545: 544 
Adding parent for node 545: 544 
Adding node 545
Message: id: 546
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 534
data_deps: 545
inputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, \'<None>\']"
  shapes: "[[106], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[222, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 544
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 546: 545 
Adding parent for node 546: 534 
Adding parent for node 546: 534 545 
Adding node 546
Message: id: 547
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 546
data_deps: 546
inputs {
  values: "[[106], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[222, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 545
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 547: 546 
Adding parent for node 547: 546 
Adding parent for node 547: 546 
Adding node 547
Message: id: 548
name: "aten::div_"
type: COMP_NODE
ctrl_deps: 534
data_deps: 547
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], [332, 621, 0, 1, 8, \'cpu\']]"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Tensor(double)\']"
}
outputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 546
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 548: 547 
Adding parent for node 548: 534 
Adding parent for node 548: 534 547 
Adding node 548
Message: id: 549
name: "c10d::_reduce_scatter_base_"
type: COMP_NODE
ctrl_deps: 534
data_deps: 548
inputs {
  values: "[[222, 298, 0, 106, 4, \'cuda:0\'], [494, 279, 0, 212, 4, \'cuda:0\'], \'<Object>\', \'<Object>\', False, -1]"
  shapes: "[[106], [212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[222, 298, 0, 106, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 547
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_reduce_scatter_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, __torch__.torch.classes.c10d.ReduceOp reduce_op, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 549: 548 
Adding parent for node 549: 534 
Adding parent for node 549: 534 548 
Adding node 549
Message: id: 550
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 549
data_deps: 549
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\'], [6, False], [\'0\', \'default_pg\'], 0, \'_reduce_scatter_base\', [], [], 0, 1, 2]"
  shapes: "[[212], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 548
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 550: 549 
Adding parent for node 550: 549 
Adding parent for node 550: 549 
Adding node 550
Message: id: 551
name: "nccl:_reduce_scatter_base"
type: COMP_NODE
ctrl_deps: 550
data_deps: 550
inputs {
  values: "[[494, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 549
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 551: 550 
Adding parent for node 551: 550 
Adding parent for node 551: 550 
Adding node 551
Message: id: 552
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 534
data_deps: 551
inputs {
  values: "[[6, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 550
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 552: 551 
Adding parent for node 552: 534 
Adding parent for node 552: 534 551 
Adding node 552
Message: id: 553
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 534
data_deps: 552
inputs {
  values: "[[41, 279, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 551
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 553: 552 
Adding parent for node 553: 534 
Adding parent for node 553: 534 552 
Adding node 553
Message: id: 554
name: "Optimizer.step#SGD.step"
type: COMP_NODE
ctrl_deps: 405
data_deps: 457
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 552
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 554: 457 
Adding parent for node 554: 405 
Adding parent for node 554: 405 457 
Adding node 554
Message: id: 555
name: "aten::_foreach_add_"
type: COMP_NODE
ctrl_deps: 554
data_deps: 554
inputs {
  values: "[[[62, 107, 0, 106, 4, \'cuda:0\']], [[222, 298, 0, 106, 4, \'cuda:0\']], -0.01]"
  shapes: "[[[106]], [[106]], []]"
  types: "[\'GenericList[Tensor(float)]\', \'GenericList[Tensor(float)]\', \'Double\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 553
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 555: 554 
Adding parent for node 555: 554 
Adding parent for node 555: 554 
Adding node 555
Message: id: 556
name: "aten::result_type"
type: COMP_NODE
ctrl_deps: 555
data_deps: 555
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], -0.01]"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[6]"
  shapes: "[[]]"
  types: "[\'Int\']"
}
attr {
  name: "rf_id"
  int64_val: 554
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 556: 555 
Adding parent for node 556: 555 
Adding parent for node 556: 555 
Adding node 556
Message: id: 557
name: "aten::item"
type: COMP_NODE
ctrl_deps: 405
data_deps: 556
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.071306]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 555
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::item(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 557: 556 
Adding parent for node 557: 405 
Adding parent for node 557: 405 556 
Adding node 557
Message: id: 558
name: "aten::_local_scalar_dense"
type: COMP_NODE
ctrl_deps: 557
data_deps: 557
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.071306]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 556
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_local_scalar_dense(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 558: 557 
Adding parent for node 558: 557 
Adding parent for node 558: 557 
Adding node 558
Message: id: 559
name: "ProfilerStep#3"
type: COMP_NODE
ctrl_deps: 1
data_deps: 558
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 557
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 559: 558 
Adding parent for node 559: 1 
Adding parent for node 559: 1 558 
Adding node 559
Message: id: 560
name: "Optimizer.zero_grad#SGD.zero_grad"
type: COMP_NODE
ctrl_deps: 559
data_deps: 559
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 558
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 560: 559 
Adding parent for node 560: 559 
Adding parent for node 560: 559 
Adding node 560
Message: id: 561
name: "FullyShardedDataParallel.forward"
type: COMP_NODE
ctrl_deps: 559
data_deps: 560
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 559
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 561: 560 
Adding parent for node 561: 559 
Adding parent for node 561: 559 560 
Adding node 561
Message: id: 562
name: "FullyShardedDataParallel._root_pre_forward"
type: COMP_NODE
ctrl_deps: 561
data_deps: 561
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 560
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 562: 561 
Adding parent for node 562: 561 
Adding parent for node 562: 561 
Adding node 562
Message: id: 563
name: "FullyShardedDataParallel._to_kwargs"
type: COMP_NODE
ctrl_deps: 562
data_deps: 562
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 561
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 563: 562 
Adding parent for node 563: 562 
Adding parent for node 563: 562 
Adding node 563
Message: id: 564
name: "FullyShardedDataParallel._pre_forward"
type: COMP_NODE
ctrl_deps: 561
data_deps: 563
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 562
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 564: 563 
Adding parent for node 564: 561 
Adding parent for node 564: 561 563 
Adding node 564
Message: id: 565
name: "FullyShardedDataParallel.rate_limiter"
type: COMP_NODE
ctrl_deps: 564
data_deps: 564
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 563
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 565: 564 
Adding parent for node 565: 564 
Adding parent for node 565: 564 
Adding node 565
Message: id: 566
name: "c10d::_allgather_base_"
type: COMP_NODE
ctrl_deps: 564
data_deps: 565
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [222, 107, 0, 106, 4, \'cuda:0\'], \'<Object>\', False, -1]"
  shapes: "[[212], [106], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 564
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_allgather_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 566: 565 
Adding parent for node 566: 564 
Adding parent for node 566: 564 565 
Adding node 566
Message: id: 567
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 566
data_deps: 566
inputs {
  values: "[[222, 107, 0, 106, 4, \'cuda:0\'], [7, False], [\'0\', \'default_pg\'], 0, \'_allgather_base\', [], [], 0, 1, 2]"
  shapes: "[[106], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 565
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 567: 566 
Adding parent for node 567: 566 
Adding parent for node 567: 566 
Adding node 567
Message: id: 568
name: "nccl:_all_gather_base"
type: COMP_NODE
ctrl_deps: 567
data_deps: 567
inputs {
  values: "[[222, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 566
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 568: 567 
Adding parent for node 568: 567 
Adding parent for node 568: 567 
Adding node 568
Message: id: 569
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 564
data_deps: 568
inputs {
  values: "[[7, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 567
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 569: 568 
Adding parent for node 569: 564 
Adding parent for node 569: 564 568 
Adding node 569
Message: id: 570
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 564
data_deps: 569
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 568
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 570: 569 
Adding parent for node 570: 564 
Adding parent for node 570: 564 569 
Adding node 570
Message: id: 571
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 570
data_deps: 570
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 569
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 571: 570 
Adding parent for node 571: 570 
Adding parent for node 571: 570 
Adding node 571
Message: id: 572
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 564
data_deps: 571
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 570
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 572: 571 
Adding parent for node 572: 564 
Adding parent for node 572: 564 571 
Adding node 572
Message: id: 573
name: "aten::split_with_sizes"
type: COMP_NODE
ctrl_deps: 564
data_deps: 572
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128, 16, 64, 4], 0]"
  shapes: "[[212], [[], [], [], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int,Int,Int]\', \'Int\']"
}
outputs {
  values: "[[[178, 135, 0, 128, 4, \'cuda:0\'], [93, 135, 128, 16, 4, \'cuda:0\'], [91, 135, 144, 64, 4, \'cuda:0\'], [651, 135, 208, 4, 4, \'cuda:0\']]]"
  shapes: "[[[128], [16], [64], [4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 571
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 573: 572 
Adding parent for node 573: 564 
Adding parent for node 573: 564 572 
Adding node 573
Message: id: 574
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 573
data_deps: 573
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [128], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[178, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 572
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 574: 573 
Adding parent for node 574: 573 
Adding parent for node 574: 573 
Adding node 574
Message: id: 575
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 573
data_deps: 574
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [16], [1], 128]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[93, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 573
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 575: 574 
Adding parent for node 575: 573 
Adding parent for node 575: 573 574 
Adding node 575
Message: id: 576
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 573
data_deps: 575
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [64], [1], 144]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 574
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 576: 575 
Adding parent for node 576: 573 
Adding parent for node 576: 573 575 
Adding node 576
Message: id: 577
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 573
data_deps: 576
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [4], [1], 208]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[651, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 575
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 577: 576 
Adding parent for node 577: 573 
Adding parent for node 577: 573 576 
Adding node 577
Message: id: 578
name: "aten::view"
type: COMP_NODE
ctrl_deps: 564
data_deps: 577
inputs {
  values: "[[178, 135, 0, 128, 4, \'cuda:0\'], [16, 8]]"
  shapes: "[[128], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[653, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 576
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 42
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 578: 577 
Adding parent for node 578: 564 
Adding parent for node 578: 564 577 
Adding node 578
Message: id: 579
name: "aten::view"
type: COMP_NODE
ctrl_deps: 564
data_deps: 578
inputs {
  values: "[[93, 135, 128, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[496, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 577
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 43
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 579: 578 
Adding parent for node 579: 564 
Adding parent for node 579: 564 578 
Adding node 579
Message: id: 580
name: "aten::view"
type: COMP_NODE
ctrl_deps: 564
data_deps: 579
inputs {
  values: "[[91, 135, 144, 64, 4, \'cuda:0\'], [4, 16]]"
  shapes: "[[64], [[], []]]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\']"
}
outputs {
  values: "[[334, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 578
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 44
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 580: 579 
Adding parent for node 580: 564 
Adding parent for node 580: 564 579 
Adding node 580
Message: id: 581
name: "aten::view"
type: COMP_NODE
ctrl_deps: 564
data_deps: 580
inputs {
  values: "[[651, 135, 208, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[152, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 579
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 45
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 581: 580 
Adding parent for node 581: 564 
Adding parent for node 581: 564 580 
Adding node 581
Message: id: 582
name: "FullyShardedDataParallel._pre_forward_prefetch"
type: COMP_NODE
ctrl_deps: 564
data_deps: 581
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 580
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 582: 581 
Adding parent for node 582: 564 
Adding parent for node 582: 564 581 
Adding node 582
Message: id: 583
name: "aten::expand_as"
type: COMP_NODE
ctrl_deps: 564
data_deps: 582
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [62, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 581
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 46
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 583: 582 
Adding parent for node 583: 564 
Adding parent for node 583: 564 582 
Adding node 583
Message: id: 584
name: "aten::expand"
type: COMP_NODE
ctrl_deps: 583
data_deps: 583
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], False]"
  shapes: "[[212], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 582
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 46
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 584: 583 
Adding parent for node 584: 583 
Adding parent for node 584: 583 
Adding node 584
Message: id: 585
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 584
data_deps: 584
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [212], [1], \'<None>\']"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'None\']"
}
outputs {
  values: "[[222, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 583
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 585: 584 
Adding parent for node 585: 584 
Adding parent for node 585: 584 
Adding node 585
Message: id: 586
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 561
data_deps: 585
inputs {
  values: "[[80, 34, 0, 256, 4, \'cuda:0\'], [653, 135, 0, 128, 4, \'cuda:0\'], [496, 135, 128, 16, 4, \'cuda:0\']]"
  shapes: "[[32, 8], [16, 8], [16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[651, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 584
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 47
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 586: 585 
Adding parent for node 586: 561 
Adding parent for node 586: 561 585 
Adding node 586
Message: id: 587
name: "aten::t"
type: COMP_NODE
ctrl_deps: 586
data_deps: 586
inputs {
  values: "[[653, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 585
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 47
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 587: 586 
Adding parent for node 587: 586 
Adding parent for node 587: 586 
Adding node 587
Message: id: 588
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 587
data_deps: 587
inputs {
  values: "[[653, 135, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 586
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 588: 587 
Adding parent for node 588: 587 
Adding parent for node 588: 587 
Adding node 588
Message: id: 589
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 588
data_deps: 588
inputs {
  values: "[[653, 135, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[222, 135, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 587
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 589: 588 
Adding parent for node 589: 588 
Adding parent for node 589: 588 
Adding node 589
Message: id: 590
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 586
data_deps: 589
inputs {
  values: "[[496, 135, 128, 16, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\'], [222, 135, 0, 128, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[16], [32, 8], [8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[651, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 588
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 48
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 590: 589 
Adding parent for node 590: 586 
Adding parent for node 590: 586 589 
Adding node 590
Message: id: 591
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 590
data_deps: 590
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[178, 166, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 589
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 591: 590 
Adding parent for node 591: 590 
Adding parent for node 591: 590 
Adding node 591
Message: id: 592
name: "aten::relu"
type: COMP_NODE
ctrl_deps: 561
data_deps: 591
inputs {
  values: "[[651, 167, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 590
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 49
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::relu(Tensor self) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 592: 591 
Adding parent for node 592: 561 
Adding parent for node 592: 561 591 
Adding node 592
Message: id: 593
name: "aten::clamp_min"
type: COMP_NODE
ctrl_deps: 592
data_deps: 592
inputs {
  values: "[[651, 167, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[222, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 591
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::clamp_min(Tensor self, Scalar min) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 593: 592 
Adding parent for node 593: 592 
Adding parent for node 593: 592 
Adding node 593
Message: id: 594
name: "aten::linear"
type: COMP_NODE
ctrl_deps: 561
data_deps: 593
inputs {
  values: "[[222, 166, 0, 512, 4, \'cuda:0\'], [334, 135, 144, 64, 4, \'cuda:0\'], [152, 135, 208, 4, 4, \'cuda:0\']]"
  shapes: "[[32, 16], [4, 16], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[91, 56, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 592
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 50
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 594: 593 
Adding parent for node 594: 561 
Adding parent for node 594: 561 593 
Adding node 594
Message: id: 595
name: "aten::t"
type: COMP_NODE
ctrl_deps: 594
data_deps: 594
inputs {
  values: "[[334, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 593
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 50
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 595: 594 
Adding parent for node 595: 594 
Adding parent for node 595: 594 
Adding node 595
Message: id: 596
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 595
data_deps: 595
inputs {
  values: "[[334, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 594
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 596: 595 
Adding parent for node 596: 595 
Adding parent for node 596: 595 
Adding node 596
Message: id: 597
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 596
data_deps: 596
inputs {
  values: "[[334, 135, 144, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 595
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 597: 596 
Adding parent for node 597: 596 
Adding parent for node 597: 596 
Adding node 597
Message: id: 598
name: "aten::addmm"
type: COMP_NODE
ctrl_deps: 594
data_deps: 597
inputs {
  values: "[[152, 135, 208, 4, 4, \'cuda:0\'], [222, 166, 0, 512, 4, \'cuda:0\'], [651, 135, 144, 64, 4, \'cuda:0\'], 1, 1]"
  shapes: "[[4], [32, 16], [16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[91, 56, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 596
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 51
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 598: 597 
Adding parent for node 598: 594 
Adding parent for node 598: 594 597 
Adding node 598
Message: id: 599
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 598
data_deps: 598
inputs {
  values: "[[1048576], 0, \'<None>\', \'cuda\', \'<None>\', \'<None>\']"
  shapes: "[[[]], [], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'Int\', \'None\', \'Device\', \'None\', \'None\']"
}
outputs {
  values: "[[93, 177, 0, 1048576, 1, \'cuda:0\']]"
  shapes: "[[1048576]]"
  types: "[\'Tensor(unsigned char)\']"
}
attr {
  name: "rf_id"
  int64_val: 597
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 599: 598 
Adding parent for node 599: 598 
Adding parent for node 599: 598 
Adding node 599
Message: id: 600
name: "FullyShardedDataParallel._post_forward"
type: COMP_NODE
ctrl_deps: 561
data_deps: 599
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 598
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 600: 599 
Adding parent for node 600: 561 
Adding parent for node 600: 561 599 
Adding node 600
Message: id: 601
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 600
data_deps: 600
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 599
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 52
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 601: 600 
Adding parent for node 601: 600 
Adding parent for node 601: 600 
Adding node 601
Message: id: 602
name: "aten::broadcast_tensors"
type: COMP_NODE
ctrl_deps: 559
data_deps: 601
inputs {
  values: "[[[91, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
outputs {
  values: "[[[91, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\']]]"
  shapes: "[[[32, 4], [32, 4]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 600
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 52
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 602: 601 
Adding parent for node 602: 559 
Adding parent for node 602: 559 601 
Adding node 602
Message: id: 603
name: "aten::mse_loss"
type: COMP_NODE
ctrl_deps: 559
data_deps: 602
inputs {
  values: "[[91, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 601
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 52
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss(Tensor self, Tensor target, int reduction=1) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 603: 602 
Adding parent for node 603: 559 
Adding parent for node 603: 559 602 
Adding node 603
Message: id: 604
name: "aten::resize_"
type: COMP_NODE
ctrl_deps: 603
data_deps: 603
inputs {
  values: "[[365, 81, 0, 128, 4, \'cuda:0\'], [], \'<None>\']"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'None\']"
}
outputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 602
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 604: 603 
Adding parent for node 604: 603 
Adding parent for node 604: 603 
Adding node 604
Message: id: 605
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 603
data_deps: 604
inputs {
  values: "[[32, 4], 6, \'<None>\', \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'None\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 603
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 605: 604 
Adding parent for node 605: 603 
Adding parent for node 605: 603 604 
Adding node 605
Message: id: 606
name: "aten::mean"
type: COMP_NODE
ctrl_deps: 603
data_deps: 605
inputs {
  values: "[[93, 167, 0, 128, 4, \'cuda:0\'], [], False, \'<None>\', [365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [], [], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[]\', \'Bool\', \'None\', \'Tensor(float)\']"
}
outputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 604
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 606: 605 
Adding parent for node 606: 603 
Adding parent for node 606: 603 605 
Adding node 606
Message: id: 607
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 606
data_deps: 606
inputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\'], [1, 1], [0, 0], \'<None>\']"
  shapes: "[[], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[77, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[1, 1]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 605
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 607: 606 
Adding parent for node 607: 606 
Adding parent for node 607: 606 
Adding node 607
Message: id: 608
name: "aten::ones_like"
type: COMP_NODE
ctrl_deps: 559
data_deps: 607
inputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 606
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 608: 607 
Adding parent for node 608: 559 
Adding parent for node 608: 559 607 
Adding node 608
Message: id: 609
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 608
data_deps: 608
inputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, 1]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 607
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 609: 608 
Adding parent for node 609: 608 
Adding parent for node 609: 608 
Adding node 609
Message: id: 610
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 609
data_deps: 609
inputs {
  values: "[[], [], 6, 0, \'cuda:0\', False]"
  shapes: "[[], [], [], [], [], []]"
  types: "[\'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 608
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 610: 609 
Adding parent for node 610: 609 
Adding parent for node 610: 609 
Adding node 610
Message: id: 611
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 608
data_deps: 610
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], 1.0]"
  shapes: "[[], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 609
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 611: 610 
Adding parent for node 611: 608 
Adding parent for node 611: 608 610 
Adding node 611
Message: id: 612
name: "autograd::engine::evaluate_function: MseLossBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 553
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 610
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 612: 553 
Adding parent for node 612: 150 
Adding parent for node 612: 150 553 
Adding node 612
Message: id: 613
name: "MseLossBackward0"
type: COMP_NODE
ctrl_deps: 612
data_deps: 612
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 611
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 52
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 613: 612 
Adding parent for node 613: 612 
Adding parent for node 613: 612 
Adding node 613
Message: id: 614
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 613
data_deps: 613
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], [91, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1]"
  shapes: "[[], [32, 4], [32, 4], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 612
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 614: 613 
Adding parent for node 614: 613 
Adding parent for node 614: 613 
Adding node 614
Message: id: 615
name: "aten::zeros_like"
type: COMP_NODE
ctrl_deps: 614
data_deps: 614
inputs {
  values: "[[91, 56, 0, 128, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'None\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 613
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 615: 614 
Adding parent for node 615: 614 
Adding parent for node 615: 614 
Adding node 615
Message: id: 616
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 615
data_deps: 615
inputs {
  values: "[[91, 56, 0, 128, 4, \'cuda:0\'], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[32, 4], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 614
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 616: 615 
Adding parent for node 616: 615 
Adding parent for node 616: 615 
Adding node 616
Message: id: 617
name: "aten::empty"
type: COMP_NODE
ctrl_deps: 616
data_deps: 616
inputs {
  values: "[[32, 4], 6, 0, \'cuda:0\', \'<None>\', 0]"
  shapes: "[[[], []], [], [], [], [], []]"
  types: "[\'GenericList[Int,Int]\', \'Int\', \'Int\', \'Device\', \'None\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 615
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 617: 616 
Adding parent for node 617: 616 
Adding parent for node 617: 616 
Adding node 617
Message: id: 618
name: "aten::zero_"
type: COMP_NODE
ctrl_deps: 615
data_deps: 617
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 616
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::zero_(Tensor(a!) self) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 618: 617 
Adding parent for node 618: 615 
Adding parent for node 618: 615 617 
Adding node 618
Message: id: 619
name: "aten::fill_"
type: COMP_NODE
ctrl_deps: 618
data_deps: 618
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 4], []]"
  types: "[\'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 617
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 619: 618 
Adding parent for node 619: 618 
Adding parent for node 619: 618 
Adding node 619
Message: id: 620
name: "aten::mse_loss_backward"
type: COMP_NODE
ctrl_deps: 614
data_deps: 619
inputs {
  values: "[[356, 186, 0, 1, 4, \'cuda:0\'], [91, 56, 0, 128, 4, \'cuda:0\'], [74, 49, 0, 128, 4, \'cuda:0\'], 1, [462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[], [32, 4], [32, 4], [], [32, 4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Int\', \'Tensor(float)\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 618
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 620: 619 
Adding parent for node 620: 614 
Adding parent for node 620: 614 619 
Adding node 620
Message: id: 621
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 620
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 619
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 621: 620 
Adding parent for node 621: 150 
Adding parent for node 621: 150 620 
Adding node 621
Message: id: 622
name: "FullyShardedDataParallel._pre_backward_hook"
type: COMP_NODE
ctrl_deps: 621
data_deps: 621
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 620
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 622: 621 
Adding parent for node 622: 621 
Adding parent for node 622: 621 
Adding node 622
Message: id: 623
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 622
data_deps: 622
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], 0, 0, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 621
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 623: 622 
Adding parent for node 623: 622 
Adding parent for node 623: 622 
Adding node 623
Message: id: 624
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 623
data_deps: 623
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], [212], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[494, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 622
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 624: 623 
Adding parent for node 624: 623 
Adding parent for node 624: 623 
Adding node 624
Message: id: 625
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 622
data_deps: 624
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], [494, 135, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[106], [212]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 623
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 625: 624 
Adding parent for node 625: 622 
Adding parent for node 625: 622 624 
Adding node 625
Message: id: 626
name: "FullyShardedDataParallel._pre_backward_prefetch"
type: COMP_NODE
ctrl_deps: 622
data_deps: 625
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 624
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 626: 625 
Adding parent for node 626: 622 
Adding parent for node 626: 622 625 
Adding node 626
Message: id: 627
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 621
data_deps: 626
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 625
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 51
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 627: 626 
Adding parent for node 627: 621 
Adding parent for node 627: 621 626 
Adding node 627
Message: id: 628
name: "aten::t"
type: COMP_NODE
ctrl_deps: 627
data_deps: 627
inputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 626
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 628: 627 
Adding parent for node 628: 627 
Adding parent for node 628: 627 
Adding node 628
Message: id: 629
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 628
data_deps: 628
inputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 627
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 629: 628 
Adding parent for node 629: 628 
Adding parent for node 629: 628 
Adding node 629
Message: id: 630
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 629
data_deps: 629
inputs {
  values: "[[651, 135, 144, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 628
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 630: 629 
Adding parent for node 630: 629 
Adding parent for node 630: 629 
Adding node 630
Message: id: 631
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 627
data_deps: 630
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [494, 135, 144, 64, 4, \'cuda:0\']]"
  shapes: "[[32, 4], [4, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[41, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 629
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 631: 630 
Adding parent for node 631: 627 
Adding parent for node 631: 627 630 
Adding node 631
Message: id: 632
name: "aten::t"
type: COMP_NODE
ctrl_deps: 627
data_deps: 631
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[32, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[494, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 630
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 632: 631 
Adding parent for node 632: 627 
Adding parent for node 632: 627 631 
Adding node 632
Message: id: 633
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 632
data_deps: 632
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[494, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 631
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 633: 632 
Adding parent for node 633: 632 
Adding parent for node 633: 632 
Adding node 633
Message: id: 634
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 633
data_deps: 633
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [4, 32], [1, 4], \'<None>\']"
  shapes: "[[32, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[494, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[4, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 632
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 634: 633 
Adding parent for node 634: 633 
Adding parent for node 634: 633 
Adding node 634
Message: id: 635
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 627
data_deps: 634
inputs {
  values: "[[494, 167, 0, 128, 4, \'cuda:0\'], [222, 166, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[4, 32], [32, 16]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 633
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 635: 634 
Adding parent for node 635: 627 
Adding parent for node 635: 627 634 
Adding node 635
Message: id: 636
name: "aten::t"
type: COMP_NODE
ctrl_deps: 627
data_deps: 635
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 634
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 636: 635 
Adding parent for node 636: 627 
Adding parent for node 636: 627 635 
Adding node 636
Message: id: 637
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 636
data_deps: 636
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[4, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 635
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 637: 636 
Adding parent for node 637: 636 
Adding parent for node 637: 636 
Adding node 637
Message: id: 638
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 637
data_deps: 637
inputs {
  values: "[[332, 223, 0, 64, 4, \'cuda:0\'], [16, 4], [1, 16], \'<None>\']"
  shapes: "[[4, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 636
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 638: 637 
Adding parent for node 638: 637 
Adding parent for node 638: 637 
Adding node 638
Message: id: 639
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 621
data_deps: 638
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 4], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[494, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[1, 4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 637
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 639: 638 
Adding parent for node 639: 621 
Adding parent for node 639: 621 638 
Adding node 639
Message: id: 640
name: "aten::view"
type: COMP_NODE
ctrl_deps: 621
data_deps: 639
inputs {
  values: "[[494, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[1, 4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 638
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 640: 639 
Adding parent for node 640: 621 
Adding parent for node 640: 621 639 
Adding node 640
Message: id: 641
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 640
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 639
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 641: 640 
Adding parent for node 641: 150 
Adding parent for node 641: 150 640 
Adding node 641
Message: id: 642
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 641
data_deps: 641
inputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 640
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 50
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 642: 641 
Adding parent for node 642: 641 
Adding parent for node 642: 641 
Adding node 642
Message: id: 643
name: "aten::t"
type: COMP_NODE
ctrl_deps: 642
data_deps: 642
inputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[16, 4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 641
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 643: 642 
Adding parent for node 643: 642 
Adding parent for node 643: 642 
Adding node 643
Message: id: 644
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 643
data_deps: 643
inputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 4], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 642
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 644: 643 
Adding parent for node 644: 643 
Adding parent for node 644: 643 
Adding node 644
Message: id: 645
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 644
data_deps: 644
inputs {
  values: "[[176, 223, 0, 64, 4, \'cuda:0\'], [4, 16], [16, 1], \'<None>\']"
  shapes: "[[16, 4], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 643
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 645: 644 
Adding parent for node 645: 644 
Adding parent for node 645: 644 
Adding node 645
Message: id: 646
name: "autograd::engine::evaluate_function: ReluBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 645
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 644
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 646: 645 
Adding parent for node 646: 150 
Adding parent for node 646: 150 645 
Adding node 646
Message: id: 647
name: "ReluBackward0"
type: COMP_NODE
ctrl_deps: 646
data_deps: 646
inputs {
  values: "[[41, 177, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 645
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 49
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 647: 646 
Adding parent for node 647: 646 
Adding parent for node 647: 646 
Adding node 647
Message: id: 648
name: "aten::threshold_backward"
type: COMP_NODE
ctrl_deps: 647
data_deps: 647
inputs {
  values: "[[41, 177, 0, 512, 4, \'cuda:0\'], [176, 166, 0, 512, 4, \'cuda:0\'], 0]"
  shapes: "[[32, 16], [32, 16], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Int\']"
}
outputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 646
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 648: 647 
Adding parent for node 648: 647 
Adding parent for node 648: 647 
Adding node 648
Message: id: 649
name: "autograd::engine::evaluate_function: AddmmBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 648
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 647
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 649: 648 
Adding parent for node 649: 150 
Adding parent for node 649: 150 648 
Adding node 649
Message: id: 650
name: "AddmmBackward0"
type: COMP_NODE
ctrl_deps: 649
data_deps: 649
inputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 648
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 48
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 650: 649 
Adding parent for node 650: 649 
Adding parent for node 650: 649 
Adding node 650
Message: id: 651
name: "aten::t"
type: COMP_NODE
ctrl_deps: 650
data_deps: 650
inputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[32, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[178, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 649
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 651: 650 
Adding parent for node 651: 650 
Adding parent for node 651: 650 
Adding node 651
Message: id: 652
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 651
data_deps: 651
inputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[32, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[178, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 650
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 652: 651 
Adding parent for node 652: 651 
Adding parent for node 652: 651 
Adding node 652
Message: id: 653
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 652
data_deps: 652
inputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\'], [16, 32], [1, 16], \'<None>\']"
  shapes: "[[32, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[178, 240, 0, 512, 4, \'cuda:0\']]"
  shapes: "[[16, 32]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 651
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 653: 652 
Adding parent for node 653: 652 
Adding parent for node 653: 652 
Adding node 653
Message: id: 654
name: "aten::mm"
type: COMP_NODE
ctrl_deps: 650
data_deps: 653
inputs {
  values: "[[178, 240, 0, 512, 4, \'cuda:0\'], [80, 34, 0, 256, 4, \'cuda:0\']]"
  shapes: "[[16, 32], [32, 8]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 652
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::mm(Tensor self, Tensor mat2) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 654: 653 
Adding parent for node 654: 650 
Adding parent for node 654: 650 653 
Adding node 654
Message: id: 655
name: "aten::t"
type: COMP_NODE
ctrl_deps: 650
data_deps: 654
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 653
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 655: 654 
Adding parent for node 655: 650 
Adding parent for node 655: 650 654 
Adding node 655
Message: id: 656
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 655
data_deps: 655
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[16, 8], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 654
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 656: 655 
Adding parent for node 656: 655 
Adding parent for node 656: 655 
Adding node 656
Message: id: 657
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 656
data_deps: 656
inputs {
  values: "[[41, 167, 0, 128, 4, \'cuda:0\'], [8, 16], [1, 8], \'<None>\']"
  shapes: "[[16, 8], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 655
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 657: 656 
Adding parent for node 657: 656 
Adding parent for node 657: 656 
Adding node 657
Message: id: 658
name: "aten::sum"
type: COMP_NODE
ctrl_deps: 649
data_deps: 657
inputs {
  values: "[[222, 240, 0, 512, 4, \'cuda:0\'], [0], True, \'<None>\']"
  shapes: "[[32, 16], [[]], [], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'Bool\', \'None\']"
}
outputs {
  values: "[[178, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[1, 16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 656
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 658: 657 
Adding parent for node 658: 649 
Adding parent for node 658: 649 657 
Adding node 658
Message: id: 659
name: "aten::view"
type: COMP_NODE
ctrl_deps: 649
data_deps: 658
inputs {
  values: "[[178, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[1, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[462, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 657
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 659: 658 
Adding parent for node 659: 649 
Adding parent for node 659: 649 658 
Adding node 659
Message: id: 660
name: "autograd::engine::evaluate_function: TBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 659
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 658
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 660: 659 
Adding parent for node 660: 150 
Adding parent for node 660: 150 659 
Adding node 660
Message: id: 661
name: "TBackward0"
type: COMP_NODE
ctrl_deps: 660
data_deps: 660
inputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 659
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 47
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 661: 660 
Adding parent for node 661: 660 
Adding parent for node 661: 660 
Adding node 661
Message: id: 662
name: "aten::t"
type: COMP_NODE
ctrl_deps: 661
data_deps: 661
inputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[8, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 660
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::t(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 662: 661 
Adding parent for node 662: 661 
Adding parent for node 662: 661 
Adding node 662
Message: id: 663
name: "aten::transpose"
type: COMP_NODE
ctrl_deps: 662
data_deps: 662
inputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\'], 0, 1]"
  shapes: "[[8, 16], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 661
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 663: 662 
Adding parent for node 663: 662 
Adding parent for node 663: 662 
Adding node 663
Message: id: 664
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 663
data_deps: 663
inputs {
  values: "[[176, 167, 0, 128, 4, \'cuda:0\'], [16, 8], [8, 1], \'<None>\']"
  shapes: "[[8, 16], [[], []], [[], []], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int,Int]\', \'GenericList[Int,Int]\', \'None\']"
}
outputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 662
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 664: 663 
Adding parent for node 664: 663 
Adding parent for node 664: 663 
Adding node 664
Message: id: 665
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 664
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 663
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 665: 664 
Adding parent for node 665: 150 
Adding parent for node 665: 150 664 
Adding node 665
Message: id: 666
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 665
data_deps: 665
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 664
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 45
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 666: 665 
Adding parent for node 666: 665 
Adding parent for node 666: 665 
Adding node 666
Message: id: 667
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 666
data_deps: 666
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[176, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 665
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 667: 666 
Adding parent for node 667: 666 
Adding parent for node 667: 666 
Adding node 667
Message: id: 668
name: "aten::view"
type: COMP_NODE
ctrl_deps: 667
data_deps: 667
inputs {
  values: "[[231, 229, 0, 4, 4, \'cuda:0\'], [4]]"
  shapes: "[[4], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[176, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[4]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 666
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 668: 667 
Adding parent for node 668: 667 
Adding parent for node 668: 667 
Adding node 668
Message: id: 669
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 668
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 667
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 669: 668 
Adding parent for node 669: 150 
Adding parent for node 669: 150 668 
Adding node 669
Message: id: 670
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 669
data_deps: 669
inputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[4, 16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 668
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 44
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 670: 669 
Adding parent for node 670: 669 
Adding parent for node 670: 669 
Adding node 670
Message: id: 671
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 670
data_deps: 670
inputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 669
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 671: 670 
Adding parent for node 671: 670 
Adding parent for node 671: 670 
Adding node 671
Message: id: 672
name: "aten::view"
type: COMP_NODE
ctrl_deps: 671
data_deps: 671
inputs {
  values: "[[651, 223, 0, 64, 4, \'cuda:0\'], [64]]"
  shapes: "[[4, 16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[231, 223, 0, 64, 4, \'cuda:0\']]"
  shapes: "[[64]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 670
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 672: 671 
Adding parent for node 672: 671 
Adding parent for node 672: 671 
Adding node 672
Message: id: 673
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 672
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 671
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 673: 672 
Adding parent for node 673: 150 
Adding parent for node 673: 150 672 
Adding node 673
Message: id: 674
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 673
data_deps: 673
inputs {
  values: "[[462, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 672
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 43
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 674: 673 
Adding parent for node 674: 673 
Adding parent for node 674: 673 
Adding node 674
Message: id: 675
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 674
data_deps: 674
inputs {
  values: "[[462, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[651, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 673
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 675: 674 
Adding parent for node 675: 674 
Adding parent for node 675: 674 
Adding node 675
Message: id: 676
name: "aten::view"
type: COMP_NODE
ctrl_deps: 675
data_deps: 675
inputs {
  values: "[[462, 252, 0, 16, 4, \'cuda:0\'], [16]]"
  shapes: "[[16], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[651, 252, 0, 16, 4, \'cuda:0\']]"
  shapes: "[[16]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 674
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 676: 675 
Adding parent for node 676: 675 
Adding parent for node 676: 675 
Adding node 676
Message: id: 677
name: "autograd::engine::evaluate_function: ViewBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 676
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 675
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 677: 676 
Adding parent for node 677: 150 
Adding parent for node 677: 150 676 
Adding node 677
Message: id: 678
name: "ViewBackward0"
type: COMP_NODE
ctrl_deps: 677
data_deps: 677
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[16, 8]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 676
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 42
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 678: 677 
Adding parent for node 678: 677 
Adding parent for node 678: 677 
Adding node 678
Message: id: 679
name: "aten::reshape"
type: COMP_NODE
ctrl_deps: 678
data_deps: 678
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 677
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::reshape(Tensor(a) self, SymInt[] shape) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 679: 678 
Adding parent for node 679: 678 
Adding parent for node 679: 678 
Adding node 679
Message: id: 680
name: "aten::view"
type: COMP_NODE
ctrl_deps: 679
data_deps: 679
inputs {
  values: "[[222, 167, 0, 128, 4, \'cuda:0\'], [128]]"
  shapes: "[[16, 8], [[]]]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\']"
}
outputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\']]"
  shapes: "[[128]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 678
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::view(Tensor(a) self, SymInt[] size) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 680: 679 
Adding parent for node 680: 679 
Adding parent for node 680: 679 
Adding node 680
Message: id: 681
name: "autograd::engine::evaluate_function: SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 150
data_deps: 680
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 679
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 681: 680 
Adding parent for node 681: 150 
Adding parent for node 681: 150 680 
Adding node 681
Message: id: 682
name: "SplitWithSizesBackward0"
type: COMP_NODE
ctrl_deps: 681
data_deps: 681
inputs {
  values: "[[462, 167, 0, 128, 4, \'cuda:0\'], [651, 252, 0, 16, 4, \'cuda:0\'], [231, 223, 0, 64, 4, \'cuda:0\'], [176, 229, 0, 4, 4, \'cuda:0\']]"
  shapes: "[[128], [16], [64], [4]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 680
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: 41
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 682: 681 
Adding parent for node 682: 681 
Adding parent for node 682: 681 
Adding node 682
Message: id: 683
name: "aten::cat"
type: COMP_NODE
ctrl_deps: 682
data_deps: 682
inputs {
  values: "[[[462, 167, 0, 128, 4, \'cuda:0\'], [651, 252, 0, 16, 4, \'cuda:0\'], [231, 223, 0, 64, 4, \'cuda:0\'], [176, 229, 0, 4, 4, \'cuda:0\']], 0]"
  shapes: "[[[128], [16], [64], [4]], []]"
  types: "[\'GenericList[Tensor(float),Tensor(float),Tensor(float),Tensor(float)]\', \'Int\']"
}
outputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 681
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::cat(Tensor[] tensors, int dim=0) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 683: 682 
Adding parent for node 683: 682 
Adding parent for node 683: 682 
Adding node 683
Message: id: 684
name: "autograd::engine::evaluate_function: torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 150
data_deps: 683
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 682
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 684: 683 
Adding parent for node 684: 150 
Adding parent for node 684: 150 683 
Adding node 684
Message: id: 685
name: "torch::autograd::AccumulateGrad"
type: COMP_NODE
ctrl_deps: 684
data_deps: 684
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 683
}
attr {
  name: "fw_parent"
  int64_val: 632
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 1
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 1
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 685: 684 
Adding parent for node 685: 684 
Adding parent for node 685: 684 
Adding node 685
Message: id: 686
name: "aten::detach"
type: COMP_NODE
ctrl_deps: 685
data_deps: 685
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[[332, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 684
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::detach(Tensor(a) self) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 686: 685 
Adding parent for node 686: 685 
Adding parent for node 686: 685 
Adding node 686
Message: id: 687
name: "detach"
type: COMP_NODE
ctrl_deps: 686
data_deps: 686
inputs {
  values: "[[222, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 685
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 687: 686 
Adding parent for node 687: 686 
Adding parent for node 687: 686 
Adding node 687
Message: id: 688
name: "FullyShardedDataParallel._post_backward_hook"
type: COMP_NODE
ctrl_deps: 684
data_deps: 687
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 686
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 688: 687 
Adding parent for node 688: 684 
Adding parent for node 688: 684 687 
Adding node 688
Message: id: 689
name: "aten::_has_compatible_shallow_copy_type"
type: COMP_NODE
ctrl_deps: 688
data_deps: 688
inputs {
  values: "[[62, 135, 0, 212, 4, \'cuda:0\'], [22, 107, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[212], [106]]"
  types: "[\'Tensor(float)\', \'Tensor(float)\']"
}
outputs {
  values: "[True]"
  shapes: "[[]]"
  types: "[\'Bool\']"
}
attr {
  name: "rf_id"
  int64_val: 687
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 689: 688 
Adding parent for node 689: 688 
Adding parent for node 689: 688 
Adding node 689
Message: id: 690
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 688
data_deps: 689
inputs {
  values: "[[27, 135, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 688
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 690: 689 
Adding parent for node 690: 688 
Adding parent for node 690: 688 689 
Adding node 690
Message: id: 691
name: "FullyShardedDataParallel._post_backward_prefetch"
type: COMP_NODE
ctrl_deps: 688
data_deps: 690
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 689
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 691: 690 
Adding parent for node 691: 688 
Adding parent for node 691: 688 690 
Adding node 691
Message: id: 692
name: "aten::chunk"
type: COMP_NODE
ctrl_deps: 688
data_deps: 691
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 2, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[332, 279, 0, 106, 4, \'cuda:0\'], [178, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 690
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 692: 691 
Adding parent for node 692: 688 
Adding parent for node 692: 688 691 
Adding node 692
Message: id: 693
name: "aten::split"
type: COMP_NODE
ctrl_deps: 692
data_deps: 692
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 106, 0]"
  shapes: "[[212], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\']"
}
outputs {
  values: "[[[332, 279, 0, 106, 4, \'cuda:0\'], [178, 279, 106, 106, 4, \'cuda:0\']]]"
  shapes: "[[[106], [106]]]"
  types: "[\'GenericList[Tensor(float),Tensor(float)]\']"
}
attr {
  name: "rf_id"
  int64_val: 691
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::split.Tensor(Tensor(a -> *) self, SymInt split_size, int dim=0) -> Tensor(a)[]"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 693: 692 
Adding parent for node 693: 692 
Adding parent for node 693: 692 
Adding node 693
Message: id: 694
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 693
data_deps: 693
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 692
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 694: 693 
Adding parent for node 694: 693 
Adding parent for node 694: 693 
Adding node 694
Message: id: 695
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 694
data_deps: 694
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 0, 0, 106, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 693
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 695: 694 
Adding parent for node 695: 694 
Adding parent for node 695: 694 
Adding node 695
Message: id: 696
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 695
data_deps: 695
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 0]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 694
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 696: 695 
Adding parent for node 696: 695 
Adding parent for node 696: 695 
Adding node 696
Message: id: 697
name: "aten::narrow"
type: COMP_NODE
ctrl_deps: 693
data_deps: 696
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 106]"
  shapes: "[[212], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[178, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 695
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 697: 696 
Adding parent for node 697: 693 
Adding parent for node 697: 693 696 
Adding node 697
Message: id: 698
name: "aten::slice"
type: COMP_NODE
ctrl_deps: 697
data_deps: 697
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], 0, 106, 212, 1]"
  shapes: "[[212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Int\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[178, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 696
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 698: 697 
Adding parent for node 698: 697 
Adding parent for node 698: 697 
Adding node 698
Message: id: 699
name: "aten::as_strided"
type: COMP_NODE
ctrl_deps: 698
data_deps: 698
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], [106], [1], 106]"
  shapes: "[[212], [[]], [[]], []]"
  types: "[\'Tensor(float)\', \'GenericList[Int]\', \'GenericList[Int]\', \'Int\']"
}
outputs {
  values: "[[178, 279, 106, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 697
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 699: 698 
Adding parent for node 699: 698 
Adding parent for node 699: 698 
Adding node 699
Message: id: 700
name: "aten::empty_like"
type: COMP_NODE
ctrl_deps: 688
data_deps: 699
inputs {
  values: "[[332, 279, 0, 106, 4, \'cuda:0\'], \'<None>\', \'<None>\', \'<None>\', False, \'<None>\']"
  shapes: "[[106], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'None\', \'None\', \'None\', \'Bool\', \'None\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 698
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 700: 699 
Adding parent for node 700: 688 
Adding parent for node 700: 688 699 
Adding node 700
Message: id: 701
name: "aten::empty_strided"
type: COMP_NODE
ctrl_deps: 700
data_deps: 700
inputs {
  values: "[[106], [1], 6, 0, \'cuda:0\', False]"
  shapes: "[[[]], [[]], [], [], [], []]"
  types: "[\'GenericList[Int]\', \'GenericList[Int]\', \'Int\', \'Int\', \'Device\', \'Bool\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 699
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 701: 700 
Adding parent for node 701: 700 
Adding parent for node 701: 700 
Adding node 701
Message: id: 702
name: "aten::div_"
type: COMP_NODE
ctrl_deps: 688
data_deps: 701
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], [332, 778, 0, 1, 8, \'cpu\']]"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Tensor(double)\']"
}
outputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 700
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 702: 701 
Adding parent for node 702: 688 
Adding parent for node 702: 688 701 
Adding node 702
Message: id: 703
name: "c10d::_reduce_scatter_base_"
type: COMP_NODE
ctrl_deps: 688
data_deps: 702
inputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\'], [651, 279, 0, 212, 4, \'cuda:0\'], \'<Object>\', \'<Object>\', False, -1]"
  shapes: "[[106], [212], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tensor(float)\', \'Object\', \'Object\', \'Bool\', \'Int\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\'], \'<Object>\']"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Object\']"
}
attr {
  name: "rf_id"
  int64_val: 701
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "c10d::_reduce_scatter_base_(Tensor output_tensor, Tensor input_tensor, __torch__.torch.classes.c10d.ProcessGroup process_group, __torch__.torch.classes.c10d.ReduceOp reduce_op, bool asyncOp, int timeout) -> (Tensor, __torch__.torch.classes.c10d.Work)"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 703: 702 
Adding parent for node 703: 688 
Adding parent for node 703: 688 702 
Adding node 703
Message: id: 704
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 703
data_deps: 703
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\'], [8, False], [\'0\', \'default_pg\'], 0, \'_reduce_scatter_base\', [], [], 0, 1, 2]"
  shapes: "[[212], [[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tensor(float)\', \'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[[462, 298, 0, 106, 4, \'cuda:0\']]"
  shapes: "[[106]]"
  types: "[\'Tensor(float)\']"
}
attr {
  name: "rf_id"
  int64_val: 702
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 704: 703 
Adding parent for node 704: 703 
Adding parent for node 704: 703 
Adding node 704
Message: id: 705
name: "nccl:_reduce_scatter_base"
type: COMP_NODE
ctrl_deps: 704
data_deps: 704
inputs {
  values: "[[651, 279, 0, 212, 4, \'cuda:0\']]"
  shapes: "[[212]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 703
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 705: 704 
Adding parent for node 705: 704 
Adding parent for node 705: 704 
Adding node 705
Message: id: 706
name: "record_param_comms"
type: COMP_NODE
ctrl_deps: 688
data_deps: 705
inputs {
  values: "[[8, False], [\'0\', \'default_pg\'], 0, \'wait\', [], [], -1, -1, 1]"
  shapes: "[[[], []], [[], []], [], [], [], [], [], [], []]"
  types: "[\'Tuple[Int,Bool]\', \'Tuple[String,String]\', \'Int\', \'String\', \'GenericList[]\', \'GenericList[]\', \'Int\', \'Int\', \'Int\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 704
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 706: 705 
Adding parent for node 706: 688 
Adding parent for node 706: 688 705 
Adding node 706
Message: id: 707
name: "aten::record_stream"
type: COMP_NODE
ctrl_deps: 688
data_deps: 706
inputs {
  values: "[[231, 279, 0, 212, 4, \'cuda:0\'], \'<Stream>\']"
  shapes: "[[212], []]"
  types: "[\'Tensor(float)\', \'Stream\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 705
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 2
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::record_stream(Tensor(a!) self, Stream s) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 707: 706 
Adding parent for node 707: 688 
Adding parent for node 707: 688 706 
Adding node 707
Message: id: 708
name: "Optimizer.step#SGD.step"
type: COMP_NODE
ctrl_deps: 559
data_deps: 611
inputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 706
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 7
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: ""
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 708: 611 
Adding parent for node 708: 559 
Adding parent for node 708: 559 611 
Adding node 708
Message: id: 709
name: "aten::_foreach_add_"
type: COMP_NODE
ctrl_deps: 708
data_deps: 708
inputs {
  values: "[[[62, 107, 0, 106, 4, \'cuda:0\']], [[462, 298, 0, 106, 4, \'cuda:0\']], -0.01]"
  shapes: "[[[106]], [[106]], []]"
  types: "[\'GenericList[Tensor(float)]\', \'GenericList[Tensor(float)]\', \'Double\']"
}
outputs {
  values: "[]"
  shapes: "[]"
  types: "[]"
}
attr {
  name: "rf_id"
  int64_val: 707
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 709: 708 
Adding parent for node 709: 708 
Adding parent for node 709: 708 
Adding node 709
Message: id: 710
name: "aten::result_type"
type: COMP_NODE
ctrl_deps: 709
data_deps: 709
inputs {
  values: "[[62, 107, 0, 106, 4, \'cuda:0\'], -0.01]"
  shapes: "[[106], []]"
  types: "[\'Tensor(float)\', \'Double\']"
}
outputs {
  values: "[6]"
  shapes: "[[]]"
  types: "[\'Int\']"
}
attr {
  name: "rf_id"
  int64_val: 708
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: -1
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 710: 709 
Adding parent for node 710: 709 
Adding parent for node 710: 709 
Adding node 710
Message: id: 711
name: "aten::item"
type: COMP_NODE
ctrl_deps: 559
data_deps: 710
inputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.070488]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 709
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 53
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::item(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 711: 710 
Adding parent for node 711: 559 
Adding parent for node 711: 559 710 
Adding node 711
Message: id: 712
name: "aten::_local_scalar_dense"
type: COMP_NODE
ctrl_deps: 711
data_deps: 711
inputs {
  values: "[[365, 81, 0, 1, 4, \'cuda:0\']]"
  shapes: "[[]]"
  types: "[\'Tensor(float)\']"
}
outputs {
  values: "[1.070488]"
  shapes: "[[]]"
  types: "[\'Double\']"
}
attr {
  name: "rf_id"
  int64_val: 710
}
attr {
  name: "fw_parent"
  int64_val: 0
}
attr {
  name: "seq_id"
  int64_val: 53
}
attr {
  name: "scope"
  int64_val: 0
}
attr {
  name: "tid"
  int64_val: 1
}
attr {
  name: "fw_tid"
  int64_val: 0
}
attr {
  name: "op_schema"
  string_val: "aten::_local_scalar_dense(Tensor self) -> Scalar"
}
attr {
  name: "is_cpu_op"
  bool_val: true
}
attr {
  name: "stream"
  int64_val: 0
}

Adding parent for node 712: 711 
Adding parent for node 712: 711 
Adding parent for node 712: 711 
Adding node 712
